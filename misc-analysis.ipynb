{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import sys\n",
    "\n",
    "import pyaudio\n",
    "from src.utils.system_utils import create_folder_structure\n",
    "from src.utils.audio_utils import record_audio_chunk, concatenate_stream, download_mp3, start_continuous_recording, crop_wav, enhance_audio_signal, speach_activity_detection\n",
    "from modules.whisper import load_model, transcribe_audio\n",
    "from modules.pyannote import embedding_cosine_similarity, get_embedding_model, get_pyannote_access_token, get_pyannote_pipeline, get_diarization_speaker_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record_audio_chunk(3)\n",
    "## declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.is_available()\n",
    "#DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chose to load model with processing device either cpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_TYPE = \"base.en\"\n",
    "#model = whisper.load_model(MODEL_TYPE, device=DEVICE)\n",
    "#print(\n",
    "#    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "#    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    "#)\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"audio/full_audio/full_audio.wav\"\n",
    "current_caption = transcribe_audio(model, file_name)\n",
    "current_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_URL = \"https://www.listennotes.com/podcast-clips/369-paul-rosolie-amazon-jungle-uncontacted-z5w0HVahToE/\" \n",
    "\n",
    "download_mp3(AUDIO_URL, \"lex-fridman.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through files\n",
    "\n",
    "import os\n",
    "directory = 'audio/chunks/'\n",
    "list_test = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    \n",
    "    if os.path.isfile(f):\n",
    "        current_caption = transcribe_audio(model, f)\n",
    "        print(\"list of data:\", current_caption[0])\n",
    "        list_test.append(current_caption[0])\n",
    "        #print(f\"file:{f} id:{current_caption['id']} \\n transcribed-text:{current_caption['text']} start: {current_caption['start']} , end: {current_caption['end']}\")\n",
    "\n",
    "current_caption[1]\n",
    "## make sure full_audio exists\n",
    "## if yes concatenate with it, if not create a full_audio with file that is empty and concatenate with it \n",
    "#concatenate_stream(\"audio/chunks/1_voice_chunk.wav\", \"audio/chunks/2_voice_chunk.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_caption[0][0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_test[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in list_test:\n",
    "    print(entry)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"lex-fridman.mp3\"\n",
    "\n",
    "current_caption = transcribe_audio(model, file_name) #only in maximum 30 seconds sliding windows.\n",
    "current_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "#pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp3_to_wav(mp3_fname, output_file_name=None): \n",
    "    sound = AudioSegment.from_mp3(mp3_fname)\n",
    "    \n",
    "    if output_file_name:\n",
    "       temp_file_name = output_file_name\n",
    "       sound.export(output_file_name, format=\"wav\")\n",
    "\n",
    "    else:\n",
    "        #create temp file\n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "        temp_file_name = temp_file.name\n",
    "        sound.export(temp_file_name, format=\"wav\")\n",
    "\n",
    "    return temp_file_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the mp3 to wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav_file_name = convert_mp3_to_wav(\"audio/podcast/lex_ai_neil_gershenfeld.mp3\", \"audio/podcast/lex_ai_neil_gershenfeld.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = AudioSegment.from_mp3(\"audio/podcast/lex_ai_neil_gershenfeld.mp3\")\n",
    "output_path = \"audio/podcast/\"\n",
    "sound.export(output_path, format=\"wav\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def initialize_folders():\n",
    "    root_folder = 'test/audio'\n",
    "    audio_subfolders = ['chunks', 'temp', 'speaker_segments', 'temp/temp_speaker_segements', 'temp/normalized_audio', 'temp/merged_audio']\n",
    "\n",
    "    for folder in audio_subfolders:\n",
    "        create_folder_structure(root_folder, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silent_1sec_wav(file_path):\n",
    "    # Create an empty audio segment (1 second of silence)\n",
    "    empty_audio = AudioSegment.silent(duration=1000, frame_rate=16000)  # 1000 milliseconds = 1 second\n",
    "\n",
    "    # Export the empty audio segment as a .wav file\n",
    "    empty_audio.export(file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_FILE_PATH = 'test/audio/temp/merged_audio/output_normalized_audio.wav'\n",
    "create_silent_1sec_wav(MERGED_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def delete_specific_file(file_path):\n",
    "    try:\n",
    "        # Attempt to delete the file\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# List all files and subdirectories in the folder\n",
    "def delete_folder_files(folder_path):\n",
    "    # List all files and subdirectories in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def copy_to(source_file, destination_folder):\n",
    "    try:\n",
    "    # Copy the file to the destination folder\n",
    "        shutil.copy(source_file, destination_folder)\n",
    "        print(f\"File copied from {source_file} to {destination_folder}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The source file {source_file} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_audio(merge_wav_path, file_path):\n",
    "    # Load the existing 'merge.wav' file\n",
    "    merge_audio = AudioSegment.from_wav(merge_wav_path)\n",
    "\n",
    "    audio_to_merge = AudioSegment.from_wav(file_path)\n",
    "    merge_audio += audio_to_merge\n",
    "\n",
    "    # Export the merged audio to 'test/temp/merge_audio/merge.wav'\n",
    "    merge_audio.export(merge_wav_path, format=\"wav\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_folder_files('audio/temp/merged_audio/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### itialize software and get all models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get .env variables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.pyannote as pyannote\n",
    "import modules.whisper as whisper\n",
    "import src.utils.audio_utils as audio_utils\n",
    "import src.utils.system_utils as system_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = './venv/env_variables.env'\n",
    "PYANNOTE_ACCESS_TOKEN = pyannote.get_pyannote_access_token(dotenv_path)\n",
    "PYANNOTE_ACCESS_TOKEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load transcription model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_TYPE = \"base.en\"\n",
    "#model = whisper.load_model(MODEL_TYPE, device=DEVICE)\n",
    "#print(\n",
    "#    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "#    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    "#)\n",
    "\n",
    "model = whisper.load_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load annotation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_pipeline = pyannote.get_pyannote_pipeline(pyannote_access_token=PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load speaker embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = pyannote.get_embedding_model(PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continous Audio recording using circular buffer to prevent segement (data) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_continuous_recording()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.whisper as whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper.test(pyannote_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget, QLabel\n",
    "from PyQt5.QtCore import Qt, QTimer\n",
    "\n",
    "\n",
    "class SoundWaveWidget(FigureCanvas):\n",
    "    def __init__(self, parent=None, width=5, height=2, dpi=100):\n",
    "        self.figure, self.ax = plt.subplots(figsize=(width, height), dpi=dpi)\n",
    "        super().__init__(self.figure)\n",
    "        self.setParent(parent)\n",
    "        self.plot_sound_wave()\n",
    "\n",
    "    def plot_sound_wave(self):\n",
    "        t = np.linspace(0, 1, 1000)\n",
    "        wave = np.sin(2 * np.pi * 440 * t)\n",
    "        self.ax.plot(t, wave)\n",
    "        self.ax.set_axis_off()\n",
    "\n",
    "\n",
    "class DarkThemeUI(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Dark UI with Sound Wave\")\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "        self.setStyleSheet(\"background-color: #222; color: white;\")\n",
    "\n",
    "        central_widget = QWidget(self)\n",
    "        self.setCentralWidget(central_widget)\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "        central_widget.setLayout(layout)\n",
    "\n",
    "        label = QLabel(\"Sound Wave Animation\", self)\n",
    "        label.setAlignment(Qt.AlignCenter)\n",
    "        label.setStyleSheet(\"font-size: 20px;\")\n",
    "\n",
    "        sound_wave_widget = SoundWaveWidget(self)\n",
    "\n",
    "        orange_button = QPushButton(\"Orange Button\", self)\n",
    "        orange_button.setStyleSheet(\"background-color: orange; color: white; font-size: 16px;\")\n",
    "        orange_button.clicked.connect(self.on_button_click)\n",
    "\n",
    "        layout.addWidget(label)\n",
    "        layout.addWidget(sound_wave_widget)\n",
    "        layout.addWidget(orange_button)\n",
    "\n",
    "    def on_button_click(self):\n",
    "        print(\"Orange button clicked!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    window = DarkThemeUI()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = 'test/audio/chunks/'\n",
    "folder_contents = os.listdir(folder_path)\n",
    "item_path = [os.path.join(folder_path, item) for item in folder_contents]\n",
    "item_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_file_list = sorted(item_path)\n",
    "sorted_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### test:\n",
    "import keyboard\n",
    "import os\n",
    "import time\n",
    "folder_path = 'test/audio/chunks/'\n",
    "ENHANCED_OUTPUT_PATH = 'test/audio/temp/normalized_audio/output_normalized_audio.wav'\n",
    "MERGED_FILE_PATH = 'test/audio/temp/merged_audio/output_normalized_audio.wav'\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kill_thread(exit_signal):\n",
    "    #global exit_signal\n",
    "    if keyboard.is_pressed(\"q\"):\n",
    "        print(\"Received 'q'. Signaling threads to exit...\")\n",
    "        exit_signal.value = True\n",
    "\n",
    "\n",
    "\n",
    "# Shared variable to signal the threads to exit\n",
    "signal = False\n",
    "\n",
    "# Create a pool of processes for f3\n",
    "max_processes = 4\n",
    "pool = multiprocessing.Pool(processes=max_processes)\n",
    "\n",
    "# Create a manager object for diarization results\n",
    "manager = multiprocessing.Manager()\n",
    "diarization_dict = manager.dict()\n",
    "exit_signal = manager.Value('b', signal)\n",
    "\n",
    "# Create an empty FIFO queue\n",
    "fifo_queue = Queue()\n",
    "\n",
    "# Create a condition variable\n",
    "condition = threading.Condition()\n",
    "\n",
    "# Create a thread for f4\n",
    "#pause_lock1 = threading.Lock()\n",
    "f4_thread = threading.Thread(target=whisper.continous_transcription, args=(diarization_dict, fifo_queue, exit_signal, condition))\n",
    "f4_thread.daemon = True  # Set the thread as a daemon so it exits when the main program exits\n",
    "f4_thread.name = \"Thread continous_transcription\"\n",
    "\n",
    "\n",
    "#keyboard_thread = threading.Thread(target=kill_thread, args=(exit_signal))\n",
    "#keyboard_thread.daemon = True # Set the thread as a daemon so it exits when the main program exits \n",
    "#keyboard_thread.name = \"Keyboard kill_thread\"\n",
    "\n",
    "\n",
    "\n",
    "# Start the f4 thread\n",
    "#f4_thread.start()\n",
    "#keyboard_thread.start()\n",
    "\n",
    "# Function to toggle the pause state of both threads\n",
    "toggle_pause = threading.Lock()\n",
    "\n",
    "# Register 'q' key press event for both threads\n",
    "#keyboard.on_press_key('q', toggle_pause)\n",
    "\n",
    "#def toggle_pause(event):\n",
    "#    with toggle_pause:\n",
    "#        if pause_lock1.locked():\n",
    "#            pause_lock1.release()\n",
    "#            print(f\"Thread 1 resumed.\")\n",
    "#        else:\n",
    "#            pause_lock1.acquire()\n",
    "#            print(f\"Thread 1 paused.\")\n",
    "\n",
    "        #if pause_lock2.locked():\n",
    "        #    pause_lock2.release()\n",
    "        #    print(f\"Thread 2 resumed.\")\n",
    "        #else:\n",
    "        #    pause_lock2.acquire()\n",
    "        #    print(f\"Thread 2 paused.\")\n",
    "\n",
    "# while condition: start from 7\n",
    "i = 0\n",
    "while True:\n",
    "    with condition:\n",
    "        folder_contents = os.listdir(folder_path)\n",
    "\n",
    "        #condition.notify_all()\n",
    "        if keyboard.is_pressed(\"q\"):\n",
    "            print(f\"thread kill\")\n",
    "            break\n",
    "        if len(folder_contents) < 0:#keyboard.is_pressed(\"q\"): #or i >= 2:\n",
    "            print(\"stop run threads\")\n",
    "            #print(\"Added items to the list. Resuming threads...\")\n",
    "            #condition.notify_all()  # Notify all waiting threads\n",
    "            # exit_signal = True\n",
    "            # Pause the thread\n",
    "            #pause_lock.acquire()\n",
    "            \n",
    "            #break\n",
    "        #else:\n",
    "        #    print(\"Added items to the list. Resuming threads...\")\n",
    "        #folder_contents = os.listdir(folder_path)\n",
    "\n",
    "        # Iterate over the current contents of the folder\n",
    "        for item in folder_contents:\n",
    "            item_path = os.path.join(folder_path, item)\n",
    "            #print(f\"File being evaluated:-> {item_path}\")\n",
    "            \n",
    "            #enhance_audio_signal(item_path, ENHANCED_OUTPUT_PATH) #clean up and enhance audio signal\n",
    "            \n",
    "            #concatenate_stream(MERGED_FILE_PATH, ENHANCED_OUTPUT_PATH, MERGED_FILE_PATH) #merge stream to previous one\n",
    "            #merge_audio(MERGED_FILE_PATH, ENHANCED_OUTPUT_PATH)\n",
    "            #diarization = apply_pyannote_pipeline()\n",
    "            #print(\"applied diarization\")\n",
    "            \n",
    "            if item_path not in fifo_queue.queue:\n",
    "                print(f\"File being added to queue:-> {item_path}\")\n",
    "                print(f\": timestamp start {time.strftime('%Y%m%d_%H%M%S')} \")\n",
    "\n",
    "                fifo_queue.put(item_path)\n",
    "                #condition.notify_all()#notify all threads\n",
    "                print(f\": timestamp end {time.strftime('%Y%m%d_%H%M%S')} \")\n",
    "                #pool.apply_async(whisper.cont_transcrpt_work, args=(item_path, item_path, diarization_dict, pyannote_pipeline))\n",
    "\n",
    "            #else:\n",
    "            #    print(f\"already in queue\")\n",
    "            \n",
    "            #diarized_speaker_info_df = get_diarization_speaker_info_df(pyannote_pipeline, ENHANCED_OUTPUT_PATH)\n",
    "            #diarized_speaker_info_df\n",
    "            \n",
    "            \n",
    "            #delete_specific_file(item_path)\n",
    "            #if i >= 2:\n",
    "            #    break\n",
    "        i += 1\n",
    "\n",
    "# Listen for 'q' key press to signal threads to exit\n",
    "#keyboard.wait('q')\n",
    "#print(\"Received 'q'. Signaling threads to exit...\")\n",
    "#exit_signal.value = True\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "\n",
    "# Wait for all processes to complete\n",
    "pool.join()\n",
    "\n",
    "\n",
    "#f4_thread.join()\n",
    "\n",
    "# Allow some time for f4 to process any remaining items in diarization_array\n",
    "time.sleep(5)  # Adjust the sleep duration as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define a list that the threads will work with\n",
    "my_list = []\n",
    "\n",
    "# Create a condition variable\n",
    "condition = threading.Condition()\n",
    "\n",
    "# Function for the first thread\n",
    "def thread1_function():\n",
    "    global my_list\n",
    "    while True:\n",
    "        with condition:\n",
    "            if not my_list:\n",
    "                print(\"Thread 1: List is empty. Pausing...\")\n",
    "                condition.wait()  # Wait for a signal to resume\n",
    "            else:\n",
    "                item = my_list.pop(0)\n",
    "                print(f\"Thread 1: Removed item {item} from the list\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Function for the second thread\n",
    "def thread2_function():\n",
    "    global my_list\n",
    "    while True:\n",
    "        with condition:\n",
    "            if not my_list:\n",
    "                print(\"Thread 2: List is empty. Pausing...\")\n",
    "                condition.wait()  # Wait for a signal to resume\n",
    "            else:\n",
    "                item = my_list.pop(0)\n",
    "                print(f\"Thread 2: Removed item {item} from the list\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Create and start the threads\n",
    "thread1 = threading.Thread(target=thread1_function)\n",
    "thread2 = threading.Thread(target=thread2_function)\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# Add items to the list to wake up the threads\n",
    "time.sleep(3)  # Sleep for a moment to ensure threads have started\n",
    "\n",
    "i = 0\n",
    "while i < 4:\n",
    "    processing_time = random.uniform(0, 1)\n",
    "    time.sleep(processing_time)\n",
    "    my_list.append(i)\n",
    "    with condition:\n",
    "        print(\"Added items to the list. Resuming threads...\")\n",
    "        condition.notify_all()  # Notify all waiting threads\n",
    "    i += 1\n",
    "\n",
    "# Keep the program running\n",
    "while True:\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### multi-processing step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiprocessing.active_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.system_utils as scutils\n",
    "\n",
    "scutils.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.pyannote as pyannote\n",
    "import src.utils.audio_utils as audio_utils\n",
    "\n",
    "embedding_model = pyannote.get_embedding_model(pyannote_access_token=PYANNOTE_ACCESS_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_embedding = pyannote.get_speaker_embedding_vector(embedding_model, 'test/audio/temp/temp_speaker_segements/SPEAKER_02/recorded_audio_crop_3.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_cosine_similarity, get_embedding_model\n",
    "speaker_embeddings = {}\n",
    "folder_path = 'test/audio/temp/temp_speaker_segements/'\n",
    "folder_contents = os.listdir(folder_path)\n",
    "for item in folder_contents:\n",
    "    item_path = os.path.join(folder_path, item)\n",
    "    speaker_files = os.listdir(item_path)\n",
    "    for speaker_file in speaker_files:\n",
    "        speaker_file_path = os.path.join(f'{item_path}/', speaker_file)\n",
    "        # check if file is not silent or durant is too short\n",
    "        df = audio_utils.speach_activity_detection(filename=speaker_file_path)\n",
    "        if (len(df.iloc[:][['start', 'stop']].values) >= 1) and (df.iloc[:][['start', 'stop']].values[0][1] > 0.1):\n",
    "            speaker_embedding = pyannote.get_speaker_embedding_vector(embedding_model, speaker_file_path)\n",
    "            if item not in speaker_embeddings: \n",
    "                speaker_embeddings[item] = []\n",
    "                speaker_embeddings[item].append(speaker_embedding)\n",
    "            else:\n",
    "                speaker_embeddings[item].append(speaker_embedding)\n",
    "            print(f'{speaker_file_path}')\n",
    "        else:\n",
    "            print(f\"delete file {speaker_file_path}\")\n",
    "            #delete_specific_file(speaker_file_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enhance audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "audio_t = 'audio/chunks/recorded_audio_20230819_231924.wav'\n",
    "enhance_audio_signal(audio_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply diarization pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_audio = 'audio/chunks/recorded_audio_20230819_231924.wav'\n",
    "\n",
    "#audio_path\n",
    "diarization = whisper.get_diarization_speaker_info_df(pyannote_pipeline, normalized_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization.iloc[0]['stop']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get diarization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarized_speaker_info_df = get_diarization_speaker_info_df(pyannote_pipeline, ENHANCED_OUTPUT_PATH)\n",
    "diarized_speaker_info_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "#    start_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.whisper as whisper\n",
    "import src.utils.audio_utils as audio_utils\n",
    "import src.utils.system_utils as system_utils\n",
    "import modules.pyannote as pyannote\n",
    "from modules.whisper import get_transcription_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'test/'\n",
    "\n",
    "system_utils.delete_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is English-only and has 71,825,408 parameters.\n"
     ]
    }
   ],
   "source": [
    "transcription_model = whisper.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\adamb\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "DOTENV_PATH = './venv/env_variables.env'\n",
    "PYANNOTE_ACCESS_TOKEN = pyannote.get_pyannote_access_token(DOTENV_PATH)\n",
    "\n",
    "\n",
    "pyannote_pipeline = pyannote.get_pyannote_pipeline(pyannote_access_token=PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_utils.enhance_audio_signal('test/audio/chunks/recorded_audio_20230819_231954.wav', 'test/audio/temp/enhanced_chunks/recorded_audio_20230819_231954.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dz_obj = pyannote.get_diarization_speaker_info_df(pyannote_pipeline, 'test/audio/temp/enhanced_chunks/recorded_audio_20230819_231954.wav')\n",
    "\n",
    "dz_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_caption = whisper.get_transcription_object(transcription_model, 'test/audio/temp/enhanced_chunks/recorded_audio_20230819_231954.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_caption[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in current_caption[1].iterrows():\n",
    "    print(f\" {float(row['start']) +10} text:{row['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "item = 'audio1.wav'\n",
    "ENHANCED_AUDIO_CHUNKS_PATH = 'test/audio/temp/enhanced_chunks/'\n",
    "ehanced_item_path = os.path.join(ENHANCED_AUDIO_CHUNKS_PATH, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehanced_item_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_caption[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"audio/temp/output_normalized_audio.wav\"\n",
    "file_name = 'audio/chunks/recorded_audio_20230819_231934.wav'\n",
    "df = audio_utils.speach_activity_detection(filename=file_name)\n",
    "if (len(df.iloc[:][['start', 'stop']].values) >= 1) and (df.iloc[:][['start', 'stop']].values[0][1] > 0.1):\n",
    "    current_caption = whisper.get_transcription_object(transcription_model, file_name)\n",
    "    print(f\"speaker: {current_caption[1].iloc[0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_caption[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from fairseq.models.wav2vec import Wav2Vec2Model\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained Wav2Vec 2.0 model as a feature extractor\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load a pre-trained CNN model (e.g., ResNet) as a feature extractor\n",
    "cnn_model = models.resnet18(pretrained=True)\n",
    "cnn_model.fc = torch.nn.Identity()  # Remove the final classification layer\n",
    "\n",
    "# Load an example speech segment (you can use your own data here)\n",
    "audio, sample_rate = torchaudio.load(\"example.wav\")\n",
    "\n",
    "# Extract features using the Wav2Vec 2.0 model\n",
    "with torch.no_grad():\n",
    "    wav2vec_features = wav2vec_model.feature_extractor(audio)\n",
    "\n",
    "# Extract features using the CNN model\n",
    "with torch.no_grad():\n",
    "    cnn_features = cnn_model(audio)\n",
    "\n",
    "# Use an RNN-based model like LSTM for sequential data\n",
    "rnn_model = torch.nn.LSTM(input_size=40, hidden_size=128, num_layers=3, batch_first=True)\n",
    "\n",
    "# Assuming audio features are 40-dimensional and have been preprocessed\n",
    "# You can use a suitable preprocessing pipeline for your data\n",
    "\n",
    "# Extract features using the RNN model\n",
    "with torch.no_grad():\n",
    "    audio = torch.unsqueeze(audio, 0)  # Add a batch dimension\n",
    "    rnn_features, _ = rnn_model(audio)\n",
    "\n",
    "# 'wav2vec_features', 'cnn_features', and 'rnn_features' now contain the speaker embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before transcription, diarize.\n",
    "================================================================\n",
    "use diarized speakers to create clips,\n",
    "create embedding from those -> add to embedding dictionary\n",
    "\n",
    "on first on second pass, repeat steps above and the extract embeddings and compare them to existing to assign to correct embeddings similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_caption[1].iloc[: 1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(current_caption[0][0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_to_enhance = \"audio/chunks/recorded_audio_20230819_231924.wav\"\n",
    "#audio 1\n",
    "#audio 5\n",
    "enhance_audio_signal(audio_to_enhance, ENHANCED_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Specify the output file path and format (e.g., MP3, WAV, etc.)\n",
    "output_path = 'audio/temp/output_normalized_audio.wav'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarized_speaker_info_df = get_diarization_speaker_info_df(diarization)\n",
    "diarized_speaker_info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_wav(file_name, start, end):\n",
    "    sound = AudioSegment.from_wav(file_name)\n",
    "    \n",
    "    sound = sound.set_channels(1).set_frame_rate(16000) #mono channel\n",
    "    \n",
    "    start_ms = start * 1000\n",
    "    end_ms = end * 1000\n",
    "    #extract the first 60000 frames = 60 seconds\n",
    "    window = sound[start_ms : end_ms]\n",
    "    \n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_SEGMENT_PATH = 'test/audio/temp/temp_speaker_segements/'\n",
    "speaker = 'SPEAKER_00'\n",
    "folder_path = f\"{SPEAKER_SEGMENT_PATH}{speaker}\"\n",
    "folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src.utils.audio_utils as audio_utils\n",
    "import time \n",
    "\n",
    "SPEAKER_SEGMENT_PATH = 'test/audio/temp/temp_speaker_segements_'\n",
    "\n",
    "speaker_embeddings = []\n",
    "def cluster_audio_chunks(diarized_speaker_info_df, SPEAKER_SEGMENT_PATH, normalized_audio_path):\n",
    "\n",
    "    speakers = diarized_speaker_info_df['speaker'].values\n",
    "    #SPEAKER_SEGMENT_PATH = 'test/audio/temp/temp_speaker_segements/'\n",
    "    #normalized_audio_path = 'test/audio/temp/normalized_audio/output_normalized_audio.wav'\n",
    "\n",
    "    for i, _ in enumerate(speakers):\n",
    "        #if folder doesn't exists then create folder\n",
    "        folder_path = f\"{SPEAKER_SEGMENT_PATH}\" #{speaker}\n",
    "        output_file_name = f\"{folder_path}/recorded_audio_crop_{time.strftime('%Y%m%d_%H%M%S')}.wav\"\n",
    "        #if not os.path.exists(folder_path):\n",
    "        #    os.makedirs(folder_path)\n",
    "        start = diarized_speaker_info_df.iloc[i]['start']\n",
    "        stop = diarized_speaker_info_df.iloc[i]['stop']\n",
    "        print(f\"start {start} stop {stop}\")\n",
    "        audio_utils.crop_wav(normalized_audio_path, output_file_name, start, stop)\n",
    "        #if len(speakers) > 1:\n",
    "            #audio_utils.crop_wav(normalized_audio_path, output_file_name, start, stop)\n",
    "        #else:\n",
    "            #just write it to the folder\n",
    "            #pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_SEGMENT_PATH = 'test/audio/temp/temp_speaker_segements/'\n",
    "normalized_audio_path = 'test/audio/temp/normalized_audio/output_normalized_audio.wav' \n",
    "cluster_audio_chunks(diarized_speaker_info_df, SPEAKER_SEGMENT_PATH, normalized_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "# Shared variable to signal the threads to exit\n",
    "exit_signal = False\n",
    "\n",
    "# Function for the first thread\n",
    "def thread_function_1():\n",
    "    global exit_signal\n",
    "    while not exit_signal:\n",
    "        # Your code for thread 1 here\n",
    "        print(\"Thread 1 is running\")\n",
    "        time.sleep(1)  # Simulate some work\n",
    "\n",
    "# Function for the second thread\n",
    "def thread_function_2():\n",
    "    global exit_signal\n",
    "    while not exit_signal:\n",
    "        # Your code for thread 2 here\n",
    "        print(\"Thread 2 is running\")\n",
    "        time.sleep(1)  # Simulate some work\n",
    "\n",
    "# Create the first thread\n",
    "thread1 = threading.Thread(target=thread_function_1)\n",
    "\n",
    "# Create the second thread\n",
    "thread2 = threading.Thread(target=thread_function_2)\n",
    "\n",
    "# Start both threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# Wait for user input to exit\n",
    "input(\"Press 'q' and Enter to exit...\")\n",
    "\n",
    "#if keyboard.is_pressed(\"q\"):\n",
    "    # Set the exit signal to True to stop the threads\n",
    "#    exit_signal = True\n",
    "exit_signal = True\n",
    "# Wait for both threads to finish\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "\n",
    "print(\"Both threads have exited.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_file = 'test/audio/cropped_speaker_segements/SPEAKER_02/recorded_audio_crop_2.wav'\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Inference\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_embedding_model(pyannote_access_token):\n",
    "    model = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                                use_auth_token=pyannote_access_token)\n",
    "\n",
    "    return model\n",
    "\n",
    "def embedding_cosine_similarity(model, speaker1_file, speaker2_file):\n",
    "    inference = Inference(model, window=\"whole\")\n",
    "    embedding1 = inference(speaker1_file)#(\"speaker1.wav\")\n",
    "    embedding2 = inference(speaker2_file)#(\"speaker2.wav\")\n",
    "\n",
    "    print(f\"embedding 1 {embedding1.reshape(1, -1)} \\n embedding 2 {embedding2.reshape(1, -1)}\")\n",
    "    # `embeddingX` is (1 x D) numpy array extracted from the file as a whole.\n",
    "\n",
    "    distance = cdist(embedding1.reshape(1, -1), embedding2.reshape(1, -1), metric=\"cosine\")[0,0]\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_embedding_model(PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "inference_model = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                              use_auth_token=PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "segmentation_model = Model.from_pretrained(\"pyannote/segmentation\", \n",
    "                              use_auth_token=PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import OverlappedSpeechDetection\n",
    "\n",
    "HYPER_PARAMETERS = {\n",
    "  # onset/offset activation thresholds\n",
    "  \"onset\": 0.5, \"offset\": 0.5,\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "\n",
    "pipeline = OverlappedSpeechDetection(segmentation=segmentation_model)\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "osd = pipeline(\"audio/chunks/recorded_audio_20230819_232014.wav\")\n",
    "osd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Inference\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "speaker1 = 'audio/chunks/recorded_audio_20230819_231934.wav'\n",
    "#speaker1 = 'test/audio/cropped_speaker_segements/SPEAKER_00/recorded_audio_crop_4.wav'\n",
    "speaker2 = 'test/audio/cropped_speaker_segements/SPEAKER_01/recorded_audio_crop_0.wav'\n",
    "\n",
    "inference = Inference(inference_model, window=\"whole\")\n",
    "embedding1 = inference(speaker1)\n",
    "embedding2 = inference(speaker2)\n",
    "\n",
    "print(embedding2.reshape(1, -1).shape)\n",
    "\n",
    "#distance = 1 - cdist(embedding1.reshape(1, -1), embedding2.reshape(1, -1), \"cosine\")[0,0]\n",
    "distance = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0,0]\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(speaker1, sr=None)\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract audio features (e.g., MFCCs) from a segment\n",
    "def extract_features(segment, target_length, sample_rate):\n",
    "    mfccs = librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=13)\n",
    "    \n",
    "    # Pad or truncate the feature vector to the target length\n",
    "    if mfccs.shape[1] < target_length:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, target_length - mfccs.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :target_length]\n",
    "    \n",
    "    return mfccs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "speaker1 = 'test/audio/cropped_speaker_segements/SPEAKER_00/recorded_audio_crop_4.wav'\n",
    "speaker2 = 'test/audio/cropped_speaker_segements/SPEAKER_02/recorded_audio_crop_3.wav'\n",
    "\n",
    "audio_data1, sr1 = librosa.load(speaker1, sr=None)\n",
    "mfcc_features1 = librosa.feature.mfcc(y=audio_data1, sr=sr1, n_mfcc=13)\n",
    "\n",
    "audio_data2, sr2 = librosa.load(speaker2, sr=None)\n",
    "# Make sure both audio files have the same number of MFCC coefficients\n",
    "mfcc_features2 = librosa.feature.mfcc(y=audio_data2, sr=sr2, n_mfcc=13)\n",
    "\n",
    "# Transpose the MFCC feature matrices to align the features correctly\n",
    "mfcc_features1 = mfcc_features1.T\n",
    "mfcc_features2 = mfcc_features2.T\n",
    "\n",
    "# Initialize a list to store cosine similarities\n",
    "similarities = []\n",
    "\n",
    "# Calculate cosine similarity for each pair of rows\n",
    "for row1, row2 in zip(mfcc_features1, mfcc_features2):\n",
    "    similarity = 1 - cosine(row1, row2)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "# Calculate the mean similarity as an overall measure\n",
    "total_similarity = np.mean(similarities)\n",
    "\n",
    "print(f\"Total Cosine Similarity: {total_similarity}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diarize accross files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_caption = transcribe_audio(model, output_path) #only in maximum 30 seconds sliding windows.\n",
    "current_caption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import OverlappedSpeechDetection\n",
    "from pyannote.audio import Model\n",
    "modelx = Model.from_pretrained(\"pyannote/segmentation\", \n",
    "                              use_auth_token=pyannote_access_token)\n",
    "\n",
    "HYPER_PARAMETERS = {\n",
    "  # onset/offset activation thresholds\n",
    "  \"onset\": 0.5, \"offset\": 0.5,\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "\n",
    "pipeline = OverlappedSpeechDetection(segmentation=modelx)\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "osd = pipeline(\"audio/chunks/recorded_audio_20230819_231954.wav\")\n",
    "osd\n",
    "# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Load an audio file (replace 'your_audio_file.wav' with your audio file)\n",
    "audio_file = 'audio/chunks/recorded_audio_20230819_231924.wav'\n",
    "y, sr = librosa.load(audio_file)\n",
    "\n",
    "# Extract MFCC features\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # You can adjust the number of MFCC coefficients\n",
    "\n",
    "# Transpose the MFCCs matrix\n",
    "mfccs = mfccs.T\n",
    "\n",
    "# Standardize the MFCC features\n",
    "scaler = StandardScaler()\n",
    "mfccs_standardized = scaler.fit_transform(mfccs)\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "inertia_values = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(mfccs_standardized)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.plot(range(1, 11), inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n",
    "\n",
    "# Based on the Elbow curve, choose the optimal number of clusters (K)\n",
    "optimal_k = 4  # Adjust this based on the plot\n",
    "\n",
    "# Create a K-means model with the optimal K\n",
    "kmeans = KMeans(n_clusters=optimal_k)\n",
    "\n",
    "# Fit the model to the MFCC data\n",
    "kmeans.fit(mfccs_standardized)\n",
    "\n",
    "# Get cluster assignments for each frame of MFCCs\n",
    "cluster_assignments = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "librosa.display.specshow(mfccs.T, sr=sr, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title(f'K-Means Clustering with {optimal_k} Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load an audio file (replace 'your_audio_file.wav' with your audio file)\n",
    "enhanced_audio_file = 'audio/temp/output_normalized_audio.wav'\n",
    "y, sr = librosa.load(enhanced_audio_file)\n",
    "\n",
    "# Extract MFCC features\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # You can adjust the number of MFCC coefficients\n",
    "\n",
    "# Transpose the MFCCs matrix\n",
    "mfccs = mfccs.T\n",
    "\n",
    "# Standardize the MFCC features\n",
    "scaler = StandardScaler()\n",
    "mfccs_standardized = scaler.fit_transform(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    labels = kmeans.fit_predict(mfccs_standardized)\n",
    "    score = silhouette_score(mfccs_standardized, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from K=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Create a DBSCAN model with appropriate parameters (eps and min_samples)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=4)\n",
    "\n",
    "# Fit the model to your data\n",
    "labels = dbscan.fit_predict(mfccs_standardized)\n",
    "\n",
    "# Get the unique cluster labels (including noise, labeled as -1)\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Calculate the number of clusters (excluding noise points labeled as -1)\n",
    "num_clusters = len(unique_labels) - 1  # Subtract 1 to exclude the noise cluster (-1)\n",
    "\n",
    "# Print the number of clusters\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "print(\"Cluster assignments:\", labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN WITH TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'audio/temp/output_normalized_audio.wav'\n",
    "audio_data, sample_rate = librosa.load(audio_file, sr=None)  # Load with the original sample rate\n",
    "\n",
    "# Define the desired frame length and step (in samples)\n",
    "frame_length = int(sample_rate * 1)  # 1-second frames\n",
    "frame_step = int(sample_rate * 1)    # 1-second overlap\n",
    "\n",
    "# Ensure that frames are of equal length by padding or truncating\n",
    "num_frames = len(audio_data) // frame_step\n",
    "frames = [audio_data[i * frame_step: i * frame_step + frame_length] for i in range(num_frames)]\n",
    "\n",
    "# Extract features from each frame (MFCCs with transposition and standardization)\n",
    "def extract_features(frame, sample_rate):\n",
    "    # Extract MFCCs with transposition\n",
    "    mfccs = librosa.feature.mfcc(y=frame, sr=sample_rate, n_mfcc=13).T\n",
    "    \n",
    "    # Standardize the MFCCs\n",
    "    scaler = StandardScaler()\n",
    "    mfccs_standardized = scaler.fit_transform(mfccs)\n",
    "    \n",
    "    # Return the entire standardized MFCCs array\n",
    "    return mfccs_standardized.flatten()\n",
    "\n",
    "# Extract features for each frame and store them in a list\n",
    "feature_vectors = [extract_features(frame, sample_rate) for frame in frames]\n",
    "\n",
    "# Apply DBSCAN to cluster the frames\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=4)\n",
    "cluster_labels = dbscan.fit_predict(feature_vectors)\n",
    "\n",
    "# Map cluster assignments back to time ranges in seconds\n",
    "frame_duration = len(audio_data) / len(frames) / sample_rate  # Duration of each frame in seconds\n",
    "time_ranges = [(i * frame_duration, (i + 1) * frame_duration) for i in range(len(frames))]\n",
    "\n",
    "# Associate cluster labels with time ranges\n",
    "cluster_assignments = list(zip(time_ranges, cluster_labels))\n",
    "\n",
    "# Print cluster assignments with time ranges in seconds\n",
    "for time_range, cluster_label in cluster_assignments:\n",
    "    print(f\"Time Range: {time_range[0]:.2f}s - {time_range[1]:.2f}s, Cluster Label: {cluster_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "print(f\"entries: {len(feature_vectors)} - features: {len(feature_vectors[0])}\")\n",
    "# Get the unique cluster labels (including noise, labeled as -1)\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "\n",
    "# Calculate the number of clusters (excluding noise points labeled as -1)\n",
    "num_clusters = len(unique_labels) - 1  # Subtract 1 to exclude the noise cluster (-1)\n",
    "\n",
    "# Print the number of clusters\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "#----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load an audio file (replace 'your_audio_file.wav' with your audio file)\n",
    "audio_file = 'audio/temp/output_normalized_audio.wav'\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Extract MFCC features\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  \n",
    "\n",
    "# Transpose the MFCCs matrix\n",
    "mfccs = mfccs.T\n",
    "\n",
    "# Standardize the MFCC features\n",
    "scaler = StandardScaler()\n",
    "mfccs_standardized = scaler.fit_transform(mfccs)\n",
    "# Create a DBSCAN model with appropriate parameters (eps and min_samples)\n",
    "#dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Fit the model to your data\n",
    "labels = dbscan.fit_predict(mfccs_standardized)\n",
    "\n",
    "# Get the unique cluster labels (including noise, labeled as -1)\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Calculate the number of clusters (excluding noise points labeled as -1)\n",
    "num_clusters = len(unique_labels) - 1  # Subtract 1 to exclude the noise cluster (-1)\n",
    "\n",
    "# Print the number of clusters\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "\n",
    "\n",
    "# Define the frame rate used for MFCC feature extraction (you need to know this)\n",
    "frame_rate = 100  # Replace with the actual frame rate used\n",
    "\n",
    "# Calculate the duration of each frame in seconds\n",
    "frame_duration = 1.0 / frame_rate\n",
    "\n",
    "# Initialize variables to track cluster timestamps\n",
    "cluster_timestamps = {}\n",
    "\n",
    "# Iterate through cluster labels and find timestamps\n",
    "for cluster_label in unique_labels:\n",
    "    if cluster_label == -1:\n",
    "        # Skip noise cluster\n",
    "        continue\n",
    "\n",
    "    # Find indices of frames belonging to the current cluster\n",
    "    cluster_indices = np.where(labels == cluster_label)[0]\n",
    "\n",
    "    # Calculate start and end timestamps for the cluster\n",
    "    start_time = cluster_indices[0] * frame_duration\n",
    "    end_time = (cluster_indices[-1] + 1) * frame_duration  # Add 1 for inclusive end time\n",
    "\n",
    "    # Store the timestamps for the cluster\n",
    "    cluster_timestamps[cluster_label] = (start_time, end_time)\n",
    "\n",
    "# Print the timestamps for each cluster\n",
    "for cluster_label, timestamps in cluster_timestamps.items():\n",
    "    start_time, end_time = timestamps\n",
    "    print(f\"Cluster {cluster_label}: Start Time {start_time:.2f}s, End Time {end_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace this with your actual audio file or data\n",
    "audio_file = \"audio/temp/output_normalized_audio.wav\"\n",
    "\n",
    "# Load the audio and compute MFCCs\n",
    "audio, sr = librosa.load(audio_file, sr=None)  # Load audio with its original sampling rate\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)  # Compute MFCCs (adjust parameters as needed)\n",
    "\n",
    "# Transpose the MFCCs matrix to have segments as rows and features as columns\n",
    "x_vectors = mfccs.T\n",
    "\n",
    "# Calculate the linkage matrix for hierarchical clustering\n",
    "linked = linkage(x_vectors, method='ward')\n",
    "\n",
    "# Plot the dendrogram to visually inspect it\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linked, p=10, truncate_mode='level', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Based on the dendrogram, select a suitable cutoff distance to determine the number of clusters\n",
    "# You can visually inspect the dendrogram and choose a distance where clusters are well-separated\n",
    "\n",
    "# For example, suppose you choose a distance of 1000 (you should adjust this based on your data)\n",
    "cutoff_distance = 2000\n",
    "\n",
    "# Use fcluster to assign segments to clusters based on the cutoff distance\n",
    "cluster_labels = fcluster(linked, t=cutoff_distance, criterion='distance')\n",
    "\n",
    "# Now, cluster_labels contains the cluster assignments for each audio segment\n",
    "\n",
    "clusters_x = []\n",
    "# Print the cluster assignments\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    clusters_x.append(label)\n",
    "    #print(f\"Segment {i+1} is in Cluster {label}\")\n",
    "\n",
    "print(f\"uniques {np.unique(clusters_x)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace this with your actual audio file or data\n",
    "audio_file = \"audio/temp/output_normalized_audio.wav\"\n",
    "\n",
    "# Load the audio and compute MFCCs\n",
    "audio, sr = librosa.load(audio_file, sr=None)  # Load audio with its original sampling rate\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)  # Compute MFCCs (adjust parameters as needed)\n",
    "\n",
    "# Transpose the MFCCs matrix to have segments as rows and features as columns\n",
    "x_vectors = mfccs.T\n",
    "\n",
    "# Calculate the linkage matrix for hierarchical clustering\n",
    "linked = linkage(x_vectors, method='ward')\n",
    "\n",
    "# Plot the dendrogram to visually inspect it\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linked, p=10, truncate_mode='level', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Based on the dendrogram, select a suitable cutoff distance to determine the number of clusters\n",
    "# You can visually inspect the dendrogram and choose a distance where clusters are well-separated\n",
    "\n",
    "# For example, suppose you choose a distance of 1000 (you should adjust this based on your data)\n",
    "cutoff_distance = 1000\n",
    "\n",
    "# Use fcluster to assign segments to clusters based on the cutoff distance\n",
    "cluster_labels = fcluster(linked, t=cutoff_distance, criterion='distance')\n",
    "\n",
    "# Calculate the total duration of the audio\n",
    "audio_duration = len(audio) / sr  # in seconds\n",
    "\n",
    "# Estimate the hop size based on the audio duration and the number of frames\n",
    "num_frames = mfccs.shape[1]\n",
    "hop_size = audio_duration / num_frames\n",
    "\n",
    "# Create a list to store cluster boundaries (start and end timestamps)\n",
    "cluster_boundaries = []\n",
    "\n",
    "# Calculate the start and end timestamps for each cluster\n",
    "current_cluster = cluster_labels[0]\n",
    "start_timestamp = 0\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if label != current_cluster:\n",
    "        end_timestamp = (i - 1) * hop_size\n",
    "        cluster_boundaries.append((start_timestamp, end_timestamp, current_cluster))\n",
    "        start_timestamp = i * hop_size\n",
    "        current_cluster = label\n",
    "\n",
    "# Add the last cluster boundary\n",
    "end_timestamp = (len(cluster_labels) - 1) * hop_size\n",
    "cluster_boundaries.append((start_timestamp, end_timestamp, current_cluster))\n",
    "\n",
    "# Sort the cluster boundaries in ascending order of start timestamps\n",
    "cluster_boundaries.sort(key=lambda x: x[0])\n",
    "\n",
    "# Print the sorted cluster boundaries\n",
    "for i, (start_time, end_time, label) in enumerate(cluster_boundaries):\n",
    "    print(f\"Cluster {label}: Start Time = {start_time:.2f} sec, End Time = {end_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the start time and duration for each cluster\n",
    "current_cluster = cluster_labels[0]\n",
    "start_timestamp = 0\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if label != current_cluster:\n",
    "        end_timestamp = i * hop_size  # End time of the current cluster\n",
    "        duration = end_timestamp - start_timestamp  # Duration of the current cluster\n",
    "        cluster_boundaries.append((start_timestamp, duration, current_cluster))\n",
    "        start_timestamp = i * hop_size\n",
    "        current_cluster = label\n",
    "\n",
    "# Calculate the duration for the last cluster\n",
    "end_timestamp = len(cluster_labels) * hop_size\n",
    "duration = end_timestamp - start_timestamp\n",
    "cluster_boundaries.append((start_timestamp, duration, current_cluster))\n",
    "\n",
    "# Sort the cluster boundaries in ascending order of start timestamps\n",
    "cluster_boundaries.sort(key=lambda x: x[0])\n",
    "\n",
    "# Print the sorted cluster boundaries\n",
    "for i, (start_time, duration, label) in enumerate(cluster_boundaries):\n",
    "    print(f\"Cluster {label}: Start Time = {start_time:.2f} sec, Duration = {duration:.2f} sec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ignore silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_to_enhance = \"audio/chunks/recorded_audio_20230819_231924.wav\"\n",
    "#audio 1\n",
    "#audio 5\n",
    "enhance_audio_signal(audio_path=audio_to_enhance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load the audio data (replace 'your_audio_file.wav' with your actual file)\n",
    "audio_file = 'audio/temp/output_normalized_audio.wav'\n",
    "audio_data, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Extract audio features (e.g., MFCCs) for clustering\n",
    "# You can use other features depending on your application\n",
    "mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "\n",
    "# Perform hierarchical clustering for different numbers of clusters\n",
    "n_clusters_range = range(1, 11)  # Adjust as needed\n",
    "bic_scores = []\n",
    "\n",
    "for n_clusters in n_clusters_range:\n",
    "    # Fit Agglomerative Clustering\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = clustering.fit_predict(np.transpose(mfcc_features))  # Transpose for feature-wise clustering\n",
    "\n",
    "    # Calculate BIC for this clustering\n",
    "    gmm = GaussianMixture(n_components=n_clusters, random_state=random_seed)\n",
    "    gmm.fit(mfcc_features.T)  # Transpose back for sample-wise Gaussian Mixture Model\n",
    "    bic_scores.append(gmm.bic(mfcc_features.T))\n",
    "\n",
    "# Find the optimal number of clusters (minimum BIC score)\n",
    "optimal_n_clusters = n_clusters_range[np.argmin(bic_scores)]\n",
    "\n",
    "# Plot BIC scores to visualize the selection\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_clusters_range, bic_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.title('BIC Score vs. Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.axvline(x=optimal_n_clusters, color='r', linestyle='--', label='Optimal Clusters')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal number of clusters\n",
    "print(f\"Optimal Number of Clusters: {optimal_n_clusters}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.audio_utils as audio_utils\n",
    "import pyaudio\n",
    "\n",
    "#determine the available devices and max input channels for recording\n",
    "#p = pyaudio.PyAudio()\n",
    "\n",
    "#for i in range(p.get_device_count()):\n",
    "#    dev = p.get_device_info_by_index(i)\n",
    "#    print((i, dev['name'], dev['maxInputChannels']))\n",
    "\n",
    "\n",
    "audio_utils.start_continuous_recording(True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for variations in an audio signal, such as changes in pitch, intensity, or other characteristics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.audio_utils as audio_utils\n",
    "audio_file = 'enhanced_audio/test_clips/cropped_SPEAKER_00_40.0.wav'  # Replace with the path to your audio file\n",
    "at = audio_utils.speach_activity_detection(audio_file)\n",
    "\n",
    "for idx, row in at.iterrows():\n",
    "    row['start']\n",
    "    audio_utils.crop_wav(audio_file, f\"enhanced_audio/non_silent/cropped_{float(row['stop'])}.wav\", float(row['start']), float(row['stop']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'enhanced_audio/chunks/recorded_audio_20230905_161155.wav'  # Replace with the path to your audio file\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Calculate the pitch variation\n",
    "pitch, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "mean_pitch = pitch[pitch > 0].mean()\n",
    "\n",
    "# Calculate the intensity variation\n",
    "intensity = librosa.feature.rms(y=y)\n",
    "\n",
    "# Plot the pitch and intensity\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(magnitudes, ref=np.max), y_axis='log', x_axis='time')\n",
    "#plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(f'Pitch Variation (Mean Pitch: {mean_pitch:.2f} Hz)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### determine segment similarity between segments\n",
    "\n",
    "a simple example of comparing audio segments based on audio features and determining if they are similar or different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to extract audio features (e.g., MFCCs) from a segment\n",
    "def extract_features(segment, target_length, sample_rate):\n",
    "    mfccs = librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=13)\n",
    "    \n",
    "    # Pad or truncate the feature vector to the target length\n",
    "    if mfccs.shape[1] < target_length:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, target_length - mfccs.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :target_length]\n",
    "    \n",
    "    return mfccs.flatten()\n",
    "\n",
    "# Function to calculate Euclidean distance between two MFCC vectors\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    print(f\"shape1:{vector1.shape} shape2:{vector2.shape}\")\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "def compute_average_similarity(current_mfcc_vector, speakers):\n",
    "    # Your MFCC vector\n",
    "    vector_1 = current_mfcc_vector #np.array(...)  # Replace with your MFCC vector\n",
    "\n",
    "    # Your dictionary of speakers and their MFCC arrays\n",
    "    #speakers = {\n",
    "    #    'Speaker1': [np.array(...), np.array(...), ...],  # Replace with MFCC arrays\n",
    "    #    'Speaker2': [np.array(...), np.array(...), ...],  # Replace with MFCC arrays\n",
    "    #    # Add more speakers and MFCC arrays as needed\n",
    "    #}\n",
    "\n",
    "    # Initialize variables to track the closest speaker and distance\n",
    "    closest_speaker = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    # Calculate the average distance for each speaker\n",
    "    for speaker, mfcc_arrays in speakers.items():\n",
    "\n",
    "        print(f\"SET HERE: {mfcc_arrays}\")\n",
    "        # Calculate the average MFCC vector for the current speaker\n",
    "        average_vector = np.mean(mfcc_arrays, axis=0)\n",
    "        \n",
    "        # Calculate the Euclidean distance between vector_1 and the average MFCC vector\n",
    "        distance = euclidean_distance(vector_1, average_vector)\n",
    "        \n",
    "        # Check if this is the closest speaker so far\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_speaker = speaker\n",
    "\n",
    "    # Assign vector_1 to the closest speaker\n",
    "    print(f\"Vector_1 belongs to {closest_speaker} with a minimum distance of {min_distance}\")\n",
    "\n",
    "\n",
    "def merge_cluster_segments(merged_speaker_segments, audio_file):\n",
    "    # Load the audio file\n",
    "    #audio_file =   # Replace with the path to your audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "\n",
    "    # Define the start and stop times of speech segments (in seconds)\n",
    "    df = speach_activity_detection(filename=audio_file)\n",
    "    timestamps = df.iloc[:][['start', 'stop']].values\n",
    "\n",
    "\n",
    "    segment_times = timestamps# time may cause issues, check later #[(0, 2), (2, 5), (5, 8)]  # Adjust as needed\n",
    "\n",
    "    # Set a fixed length for the feature vectors\n",
    "    target_length = 1000  # You can adjust this based on your needs\n",
    "\n",
    "\n",
    "    # Initialize a dictionary to store segments for each speaker\n",
    "    speaker_segments = {}\n",
    "    speaker_segments_timestamps = {}\n",
    "    current_speaker = None  # Keep track of the current speaker\n",
    "\n",
    "    test_segment = y[int(segment_times[0][0] * sr):int(segment_times[0][1] * sr)]\n",
    "    print(f\"start: {int(segment_times[0][0] * sr)} stop: {int(segment_times[0][1] * sr)} \\n seg: \\n{test_segment}\")\n",
    "    # Compare speech segments\n",
    "    for i, (start, stop) in enumerate(segment_times):\n",
    "        segment = y[int(start * sr):int(stop * sr)]\n",
    "        print(f\"pass {segment} \\n\\n\")\n",
    "        if i > 0:\n",
    "            print(f\"INDEX {i} ---- start; {int(segment_times[i-1][0] * sr)} stop: {int(segment_times[i-1][1] * sr)}\")\n",
    "            previous_segment = y[int(segment_times[i-1][0] * sr):int(segment_times[i-1][1] * sr)]\n",
    "            \n",
    "            # Extract features from the current and previous segments\n",
    "            features_current = extract_features(segment, target_length, sr)\n",
    "            features_previous = extract_features(previous_segment, target_length, sr)\n",
    "            print(\"previous mfccs\", features_previous)\n",
    "            # Compute a similarity measure (e.g., cosine similarity) between the feature vectors        \n",
    "            similarity = np.dot(features_current, features_previous) / (np.linalg.norm(features_current) * np.linalg.norm(features_previous))\n",
    "            \n",
    "            print(f\"Segment {i}: Similarity to previous segment: {similarity:.2f}\")\n",
    "\n",
    "\n",
    "            #compute_average_similarity(features_current, merged_speaker_segments)\n",
    "\n",
    "            # You can add logic here to determine if the segments are similar or different based on the similarity measure.\n",
    "            \n",
    "            # Check if the similarity is above 0.5 to group segments under the same speaker\n",
    "            if similarity > 0.5:\n",
    "                if current_speaker is None:\n",
    "                    print(f\"new speaker: \")\n",
    "                    current_speaker = f\"speaker_{len(speaker_segments) + 1}\"\n",
    "                if current_speaker not in speaker_segments:\n",
    "                    speaker_segments[current_speaker] = []\n",
    "                    speaker_segments_timestamps[current_speaker] = []\n",
    "                if i == 1 and similarity > 0.5:\n",
    "                    print(f\"segment!!!!:0 {segment_times[0]}\")\n",
    "                    print(f\" segg {test_segment.shape} \\n\\nprev_segg: {previous_segment.shape}\")\n",
    "                    if np.array_equal(test_segment, previous_segment):\n",
    "                        print(f\"these two are the same\")\n",
    "                        previous_start = segment_times[0][0]\n",
    "                        previous_stop = segment_times[0][1]\n",
    "                        speaker_segments_timestamps[current_speaker].append((previous_start, previous_stop))\n",
    "                        speaker_segments[current_speaker].append(features_previous)\n",
    "\n",
    "                speaker_segments_timestamps[current_speaker].append((start, stop))\n",
    "                speaker_segments[current_speaker].append(features_current)\n",
    "            else:\n",
    "                current_speaker = f\"speaker_{len(speaker_segments) + 1}\"\n",
    "                if current_speaker not in speaker_segments:\n",
    "                    speaker_segments[current_speaker] = []\n",
    "                    speaker_segments_timestamps[current_speaker] = []\n",
    "                speaker_segments_timestamps[current_speaker].append((start, stop))\n",
    "                speaker_segments[current_speaker].append(features_current)\n",
    "                #current_speaker = None  # Reset the current speaker\n",
    "\n",
    "    # Post-processing step: Merge segments that belong to the same speaker\n",
    "\n",
    "    current_speaker = None\n",
    "    for speaker, segments in speaker_segments_timestamps.items():\n",
    "        if current_speaker is None:\n",
    "            current_speaker = speaker\n",
    "            merged_speaker_segments[current_speaker] = segments\n",
    "        else:\n",
    "            last_segment = merged_speaker_segments[current_speaker][-1]\n",
    "            if segments[0][0] - last_segment[1] < 1.0:  # Adjust the threshold as needed\n",
    "                merged_speaker_segments[current_speaker].extend(segments)\n",
    "            else:\n",
    "                current_speaker = speaker\n",
    "                merged_speaker_segments[current_speaker] = segments\n",
    "\n",
    "    # The `merged_speaker_segments` dictionary now contains merged segments grouped by speakers\n",
    "    print(\"Merged Speaker Segments:\")\n",
    "    for speaker, segments in merged_speaker_segments.items():\n",
    "        print(f\"{speaker}: {segments}\")\n",
    "    return merged_speaker_segments\n",
    "\n",
    "\n",
    "merged_speaker_segments = {}\n",
    "\n",
    "normalised_audio = 'audio/temp/output_normalized_audio.wav'\n",
    "\n",
    "directory = 'audio/chunks/'\n",
    "merge_speakers = {}\n",
    "idx = 0\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    \n",
    "    if os.path.isfile(f) and f.endswith(\".wav\"):\n",
    "        #audio_to_enhance = f#\"audio/chunks/recorded_audio_20230819_231924.wav\"\n",
    "        #audio 1\n",
    "        #audio 5\n",
    "        if idx  >= 2:\n",
    "            break\n",
    "        \n",
    "        enhance_audio_signal(audio_path=f)\n",
    "\n",
    "        merge_speakers = merge_cluster_segments(merged_speaker_segments, normalised_audio)\n",
    "        idx += 1 \n",
    "        print( f\"speaker keys: {merge_speakers.keys()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do tomorrow is to add a mechanism that takes the returned speaker clusters and every loop resegment the clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'audio/chunks/recorded_audio_20230819_231924.wav'\n",
    "y, sr = librosa.load(audio_file, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using noisereduce\n",
    "from noisereduce import reduce_noise\n",
    "\n",
    "reduced_audio = reduce_noise(y=y, sr=sr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------\n",
    "# UPDATED CLUSTERING\n",
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.audio_utils as audio_utils\n",
    "import modules.pyannote as pyannote\n",
    "import modules.whisper as whisper\n",
    "import src.utils.system_utils as system_utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is English-only and has 71,825,408 parameters.\n"
     ]
    }
   ],
   "source": [
    "transcription_model  = whisper.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press 'q' to stop.\n",
      "Recording stopped.\n"
     ]
    }
   ],
   "source": [
    "#audio_utils.start_continuous_recording(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = 'enhanced_audio/chunks/recorded_audio_20230905_205810.wav'\n",
    "transcript_obj = whisper.get_transcription_object(transcription_model, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seek</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>temperature</th>\n",
       "      <th>avg_logprob</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>no_speech_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.44</td>\n",
       "      <td>What's for the people? I don't know how many ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>1.168224</td>\n",
       "      <td>0.383926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.80</td>\n",
       "      <td>to you.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>1.168224</td>\n",
       "      <td>0.383926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.64</td>\n",
       "      <td>Skip. What's he really like?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>1.168224</td>\n",
       "      <td>0.383926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  seek  start   end                                               text   \n",
       "0   0     0   0.00  6.44   What's for the people? I don't know how many ...  \\\n",
       "1   1     0   6.44  6.80                                            to you.   \n",
       "2   2     0   6.80  9.64                       Skip. What's he really like?   \n",
       "\n",
       "   temperature  avg_logprob  compression_ratio  no_speech_prob  \n",
       "0          0.0    -0.555832           1.168224        0.383926  \n",
       "1          0.0    -0.555832           1.168224        0.383926  \n",
       "2          0.0    -0.555832           1.168224        0.383926  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTENV_PATH = './venv/env_variables.env'\n",
    "PYANNOTE_ACCESS_TOKEN = pyannote.get_pyannote_access_token(DOTENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\adamb\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "pyannote_pipeline = pyannote.get_pyannote_pipeline(pyannote_access_token=PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2.33</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.08</td>\n",
       "      <td>4.56</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  stop     speaker\n",
       "0   0.01  2.33  SPEAKER_00\n",
       "1   3.08  4.56  SPEAKER_00"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization = pyannote.get_diarization_speaker_info_df(pyannote_pipeline, audio)\n",
    "diarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seek</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>temperature</th>\n",
       "      <th>avg_logprob</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>no_speech_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.44</td>\n",
       "      <td>What's for the people? I don't know how many ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>1.168224</td>\n",
       "      <td>0.383926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.80</td>\n",
       "      <td>to you.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>1.168224</td>\n",
       "      <td>0.383926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.64</td>\n",
       "      <td>Skip. What's he really like?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.555832</td>\n",
       "      <td>1.168224</td>\n",
       "      <td>0.383926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  seek  start   end                                               text   \n",
       "0   0     0   0.00  6.44   What's for the people? I don't know how many ...  \\\n",
       "1   1     0   6.44  6.80                                            to you.   \n",
       "2   2     0   6.80  9.64                       Skip. What's he really like?   \n",
       "\n",
       "   temperature  avg_logprob  compression_ratio  no_speech_prob  \n",
       "0          0.0    -0.555832           1.168224        0.383926  \n",
       "1          0.0    -0.555832           1.168224        0.383926  \n",
       "2          0.0    -0.555832           1.168224        0.383926  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_obj[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#find overlapping speakers\n",
    "def transcription_without_overlapping_speakers(diarization, transcript_obj):\n",
    "    overlaps_list = []\n",
    "    for _, row in diarization.iterrows():\n",
    "        start = row['start']\n",
    "        stop = row['stop']\n",
    "        speaker = row['speaker']\n",
    "\n",
    "        xx_inds = ~((transcript_obj[1]['end'] < start) | (transcript_obj[1]['start'] > stop))\n",
    "        overlapped_text = transcript_obj[1].loc[xx_inds, :]\n",
    "        overlapped_text['speaker_start'] = start\n",
    "        overlapped_text['speaker_end'] = stop\n",
    "        overlapped_text['speaker'] = speaker\n",
    "        overlaps_list.append(overlapped_text)\n",
    "\n",
    "    #compute overlap duration\n",
    "    columns_to_drop = ['seek', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob']\n",
    "    all_overlaps = pd.concat(overlaps_list)\n",
    "    all_overlaps['max_start'] = np.maximum(all_overlaps['start'], \n",
    "                                        all_overlaps['speaker_start'])\n",
    "    all_overlaps['min_end'] = np.minimum(all_overlaps['end'],\n",
    "                                        all_overlaps['speaker_end'])\n",
    "    all_overlaps['overlap_duration'] = all_overlaps['min_end'] - all_overlaps['max_start']\n",
    "\n",
    "    all_overlaps.drop(columns=columns_to_drop, inplace=True)\n",
    "    all_overlaps = all_overlaps.reset_index(drop=True)\n",
    "\n",
    "    #pick only one text/speaker combination for each text\n",
    "    max_overlap_idx = all_overlaps.groupby('id')['overlap_duration'].idxmax()\n",
    "    all_overlaps = all_overlaps.loc[max_overlap_idx, :]\n",
    "    return all_overlaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4], [6, 7, 8]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = [[1, 2, 4]]\n",
    "arr2 = [6, 7, 8]\n",
    "arr1.append(arr2)\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'test/audio' created.\n",
      "Folder structure created successfully.\n",
      "Folder structure created successfully.\n",
      "Folder structure created successfully.\n",
      "Folder structure created successfully.\n",
      "Folder structure created successfully.\n"
     ]
    }
   ],
   "source": [
    "ROOT_FOLDER = 'test/audio'\n",
    "audio_subfolders = ['chunks', 'temp', 'speaker_segments', 'temp/speaker_segements', 'temp/normalized_audio']\n",
    "\n",
    "for folder in audio_subfolders:\n",
    "    system_utils.create_folder_structure(ROOT_FOLDER, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enh_path = 'enhanced_audio/has_perm//'\n",
    "enhanced_files = os.listdir(enh_path)\n",
    "for i, file in enumerate(enhanced_files):\n",
    "    print(\"================================================================\\n\")\n",
    "    file_name = os.path.join(enh_path, file)\n",
    "    audio_utils.enhance_audio_signal(file_name, f'enhanced_audio/enhanced/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155457.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155507.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155517.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155527.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155537.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155547.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155557.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155607.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_155617.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_164623.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_164633.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_164643.wav' has read permissions.\n",
      "File 'enhanced_audio/has_perm/recorded_audio_20230906_164653.wav' has read permissions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i, path in enumerate(os.listdir('enhanced_audio/has_perm/')):\n",
    "    file_path = os.path.join('enhanced_audio/has_perm/', path)\n",
    "    if os.access(file_path, os.R_OK):\n",
    "        print(f\"File '{file_path}' has read permissions.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not have read permissions. Attempting to change permissions.\")\n",
    "        os.chmod(file_path, 0o644) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 10.0\n",
      " speaker SPEAKER_00:  human voices and translate them in real time as well as keeping track of who is \n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 5.84\n",
      " speaker SPEAKER_00:  talking and when they are talking. So like you can see the software, what is \n",
      "start: 5.84 - stop: 11.44\n",
      " speaker SPEAKER_00:  what it is doing actually in the background, is it's taking \n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 6.88\n",
      " speaker SPEAKER_00:  from the stream that I'm currently generating from the audio stream that I'm currently generating it is creating \n",
      "start: 7.5200000000000005 - stop: 9.52\n",
      " speaker SPEAKER_00:  10 seconds clips \n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 9.88\n",
      " speaker SPEAKER_00:  every of course 10 seconds, that it will then use to one perform speaker diarization \n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 7.36\n",
      " speaker SPEAKER_00:  of 1776, the declaration of the 13th United States of America, \n",
      "start: 7.36 - stop: 10.6\n",
      " speaker SPEAKER_00:  won in the course of human events. \n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 6.5\n",
      " speaker SPEAKER_00:  It becomes necessary for one people to dissolve the political bends which are connected them with another. \n",
      "start: 6.5 - stop: 10.0\n",
      " speaker SPEAKER_00:  And to assume among the powers of the Earth, the surprise of the Earth, \n",
      "================================================================\n",
      "\n",
      "================================================================\n",
      "\n",
      "start: 0.0 - stop: 6.24\n",
      " speaker SPEAKER_00:  and equal station to which the laws of nature and of nature's goals entitle them. \n",
      "start: 6.24 - stop: 9.84\n",
      " speaker SPEAKER_00:  Addison respect to the opinions of mankind requires that. \n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "transcription_object_list = []\n",
    "enh_path = 'enhanced_audio/enhanced/'\n",
    "enhanced_files = os.listdir(enh_path)\n",
    "\n",
    "for i, file in enumerate(enhanced_files):\n",
    "    print(\"================================================================\\n\")\n",
    "    file_name = os.path.join(enh_path, file)\n",
    "    #file_name = 'enhanced_audio/enhanced/recorded_audio_20230906_155507.wav'\n",
    "    #audio_utils.enhance_audio_signal(file_name, f'enhanced_audio/enhanced/{file}')\n",
    "    diarization = pyannote.get_diarization_speaker_info_df(pyannote_pipeline, file_name)\n",
    "    transcript_obj = whisper.get_transcription_object(transcription_model, file_name)\n",
    "    all_overlaps = transcription_without_overlapping_speakers(diarization, transcript_obj)\n",
    "    \n",
    "    transcription_object_list.append(all_overlaps)\n",
    "    \n",
    "    entries_count = all_overlaps.shape[0]\n",
    "    for i, row in all_overlaps.iterrows():\n",
    "        start = row['start']\n",
    "        stop = row['end']\n",
    "        text = row['text']\n",
    "        speaker = row['speaker']\n",
    "\n",
    "        \n",
    "        print(f\"start: {start} - stop: {stop}\\n speaker {speaker}: {text} \")\n",
    "    pyannote.cluster_audio_chunks(all_overlaps, 'enhanced_audio/non_silent/', f'enhanced_audio/enhanced/{file}', [])\n",
    "    print(\"================================================================\\n\")\n",
    "    #print(row['text'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_start</th>\n",
       "      <th>speaker_end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>max_start</th>\n",
       "      <th>min_end</th>\n",
       "      <th>overlap_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.84</td>\n",
       "      <td>talking and when they are talking. So like yo...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.84</td>\n",
       "      <td>11.44</td>\n",
       "      <td>what it is doing actually in the background, ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>5.84</td>\n",
       "      <td>9.99</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  start    end                                               text   \n",
       "0   0   0.00   5.84   talking and when they are talking. So like yo...  \\\n",
       "1   1   5.84  11.44   what it is doing actually in the background, ...   \n",
       "\n",
       "   speaker_start  speaker_end     speaker  max_start  min_end   \n",
       "0           0.01         9.99  SPEAKER_00       0.01     5.84  \\\n",
       "1           0.01         9.99  SPEAKER_00       5.84     9.99   \n",
       "\n",
       "   overlap_duration  \n",
       "0              5.83  \n",
       "1              4.15  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription_object_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_start</th>\n",
       "      <th>speaker_end</th>\n",
       "      <th>clock_counter</th>\n",
       "      <th>file_path</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>years ago, the light has been seen ever since.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ghost lights or spook lights are seen all ove...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to be the spirits of the dead.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from the stream that I'm currently generating...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 seconds clips</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It becomes necessary for one people to dissol...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>20.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>And to assume among the powers of the Earth, ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>20.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It becomes necessary for one people to dissol...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>20.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>And to assume among the powers of the Earth, ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>20.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>from the stream that I'm currently generating...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10 seconds clips</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from the stream that I'm currently generating...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10 seconds clips</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>10.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>years ago, the light has been seen ever since.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ghost lights or spook lights are seen all ove...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>to be the spirits of the dead.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>years ago, the light has been seen ever since.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ghost lights or spook lights are seen all ove...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>to be the spirits of the dead.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>human voices and translate them in real time ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>70.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>human voices and translate them in real time ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>70.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Suddenly, a mysterious orb of light appears i...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the ground.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>You get the sense that the light is aware of ...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>It darts around, beckoning you.</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>90.0</td>\n",
       "      <td>test/audio/temp/speaker_segements/crop_at_end_...</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  speaker_start   \n",
       "0      years ago, the light has been seen ever since.           0.01  \\\n",
       "1    Ghost lights or spook lights are seen all ove...           0.01   \n",
       "2                      to be the spirits of the dead.           0.01   \n",
       "3    from the stream that I'm currently generating...           0.01   \n",
       "4                                    10 seconds clips           0.01   \n",
       "5    It becomes necessary for one people to dissol...           0.01   \n",
       "6    And to assume among the powers of the Earth, ...           0.01   \n",
       "7    It becomes necessary for one people to dissol...           0.01   \n",
       "8    And to assume among the powers of the Earth, ...           0.01   \n",
       "9    from the stream that I'm currently generating...           0.01   \n",
       "10                                   10 seconds clips           0.01   \n",
       "11   from the stream that I'm currently generating...           0.01   \n",
       "12                                   10 seconds clips           0.01   \n",
       "13     years ago, the light has been seen ever since.           0.01   \n",
       "14   Ghost lights or spook lights are seen all ove...           0.01   \n",
       "15                     to be the spirits of the dead.           0.01   \n",
       "16     years ago, the light has been seen ever since.           0.01   \n",
       "17   Ghost lights or spook lights are seen all ove...           0.01   \n",
       "18                     to be the spirits of the dead.           0.01   \n",
       "19   human voices and translate them in real time ...           0.01   \n",
       "20   human voices and translate them in real time ...           0.01   \n",
       "21   Suddenly, a mysterious orb of light appears i...           0.01   \n",
       "22                                        the ground.           0.01   \n",
       "23   You get the sense that the light is aware of ...           0.01   \n",
       "24                    It darts around, beckoning you.           0.01   \n",
       "\n",
       "    speaker_end  clock_counter   \n",
       "0          9.99            0.0  \\\n",
       "1          9.99            0.0   \n",
       "2          9.99            0.0   \n",
       "3          9.99           10.0   \n",
       "4          9.99           10.0   \n",
       "5          9.99           20.0   \n",
       "6          9.99           20.0   \n",
       "7          9.99           20.0   \n",
       "8          9.99           20.0   \n",
       "9          9.99           10.0   \n",
       "10         9.99           10.0   \n",
       "11         9.99           10.0   \n",
       "12         9.99           10.0   \n",
       "13         9.99            0.0   \n",
       "14         9.99            0.0   \n",
       "15         9.99            0.0   \n",
       "16         9.99            0.0   \n",
       "17         9.99            0.0   \n",
       "18         9.99            0.0   \n",
       "19         9.99           70.0   \n",
       "20         9.99           70.0   \n",
       "21         9.99           90.0   \n",
       "22         9.99           90.0   \n",
       "23         9.99           90.0   \n",
       "24         9.99           90.0   \n",
       "\n",
       "                                            file_path     speaker  \n",
       "0   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "1   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "2   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "3   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "4   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "5   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_02  \n",
       "6   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_02  \n",
       "7   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "8   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "9   test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_02  \n",
       "10  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_02  \n",
       "11  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "12  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "13  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "14  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "15  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "16  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "17  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "18  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_03  \n",
       "19  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "20  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "21  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "22  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "23  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  \n",
       "24  test/audio/temp/speaker_segements/crop_at_end_...  SPEAKER_01  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'test/transcripts/data_20230908155122.csv'\n",
    "df = pd.read_csv(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  stop     speaker\n",
       "0   0.01  9.99  SPEAKER_00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_start</th>\n",
       "      <th>speaker_end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>max_start</th>\n",
       "      <th>min_end</th>\n",
       "      <th>overlap_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.24</td>\n",
       "      <td>and equal station to which the laws of nature...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.24</td>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.24</td>\n",
       "      <td>9.84</td>\n",
       "      <td>Addison respect to the opinions of mankind re...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>6.24</td>\n",
       "      <td>9.84</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  start   end                                               text   \n",
       "0   0   0.00  6.24   and equal station to which the laws of nature...  \\\n",
       "1   1   6.24  9.84   Addison respect to the opinions of mankind re...   \n",
       "\n",
       "   speaker_start  speaker_end     speaker  max_start  min_end   \n",
       "0           0.01         9.99  SPEAKER_00       0.01     6.24  \\\n",
       "1           0.01         9.99  SPEAKER_00       6.24     9.84   \n",
       "\n",
       "   overlap_duration  \n",
       "0              6.23  \n",
       "1              3.60  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0\n",
      "1    2.0\n",
      "Name: start, dtype: float64\n",
      "0    2.0\n",
      "1    3.0\n",
      "Name: end, dtype: float64\n",
      "0     type and Vikings to see if that comes up.\n",
      "1                                 There you go.\n",
      "Name: text, dtype: object\n",
      "================================================================\n",
      "0    0.0\n",
      "1    2.0\n",
      "Name: start, dtype: float64\n",
      "0    2.0\n",
      "1    3.0\n",
      "Name: end, dtype: float64\n",
      "0     type and Vikings to see if that comes up.\n",
      "1                                 There you go.\n",
      "Name: text, dtype: object\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_df = all_overlaps[(all_overlaps['speaker_start'] == 0.01) & (all_overlaps['speaker_end'] == 2.99)]\n",
    "last_instance = filtered_df#.iloc[-1]\n",
    "for k, v in last_instance.iterrows():\n",
    "    print(last_instance['start'])\n",
    "    print(last_instance['end']) \n",
    "    print(last_instance['text'])\n",
    "    print(\"================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\adamb\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\ef54f1b59c9fbef89ea2bab0e27b4a5dd9cfce36\\pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\adamb\\.cache\\torch\\pyannote\\models--pyannote--embedding\\snapshots\\ef54f1b59c9fbef89ea2bab0e27b4a5dd9cfce36\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = pyannote.get_embedding_model(PYANNOTE_ACCESS_TOKEN)\n",
    "\n",
    "#guest_1 = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__1_20230906_200743.wav'\n",
    "#guest_2 = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__1_20230906_200819.wav'\n",
    "#guest_3 = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__2_20230906_200807.wav'\n",
    "#guest_4 = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__2_20230906_200842.wav'\n",
    "\n",
    "#pat_1 = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__2_20230906_200931.wav'\n",
    "#pat_2 = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_01__3_20230906_200923.wav'\n",
    "#sim = 1 - pyannote.embedding_cosine_similarity(embedding_model, guest_3, guest_4)\n",
    "#sim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  0\n",
      "i  1\n",
      "i  2\n",
      "i  3\n",
      "i  4\n",
      "i  5\n",
      "i  6\n",
      "i  7\n",
      "i  8\n",
      "i  9\n",
      "i  10\n",
      "i  11\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:  3   ['test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223245_s1.wav_SPEAKER_00__0_20230908_174952.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223255_s1.wav_SPEAKER_00__1_20230908_175101.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223305_s1.wav_SPEAKER_00__1_20230908_175112.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223315_s1.wav_SPEAKER_00__0_20230908_175049.wav'] \n",
      "\n",
      "cluster:  2   ['test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223415_s2.wav_SPEAKER_00__1_20230908_175057.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223425_s2.wav_SPEAKER_00__1_20230908_175053.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230907_223435_s2.wav_SPEAKER_00__1_20230908_175108.wav'] \n",
      "\n",
      "cluster:  1   ['test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230908_133604_s3.wav_SPEAKER_00__2_20230908_175120.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230908_133614_s3.wav_SPEAKER_00__3_20230908_175017.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230908_133624_s3.wav_SPEAKER_01__1_20230908_175105.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230908_133634_s3.wav_SPEAKER_00__2_20230908_175116.wav', 'test/audio/temp/speaker_segements/crop_at_end_recorded_audio_20230908_133644_s3.wav_SPEAKER_00__2_20230908_175038.wav'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from models.cluster_point import ClusterPoint\n",
    "\n",
    "CLUSTER_ROOT = 'test/audio/temp/speaker_segements/'#'enhanced_audio/test_clips/'\n",
    "\n",
    "def init_embeddings_cluster(CLUSTER_ROOT, embedding_model):\n",
    "    cluster_point = ClusterPoint()\n",
    "    cluster_files = os.listdir(CLUSTER_ROOT) \n",
    "    embeddings_dict = {}\n",
    "    for i, file in enumerate(cluster_files):\n",
    "        file_path = os.path.join(CLUSTER_ROOT, file)\n",
    "        print(\"i \", i)\n",
    "        if file_path not in embeddings_dict:\n",
    "            embeddings_dict[file_path] = pyannote.get_speaker_embedding_vector(embedding_model, file_path)\n",
    "\n",
    "    embedding_arrays = np.array(list(embeddings_dict.values()))\n",
    "    cosine_similarity_matrix = cosine_similarity(embedding_arrays, embedding_arrays)\n",
    "\n",
    "    # Compute linkage matrix\n",
    "    linkage_matrix = linkage(1 - cosine_similarity_matrix, method='ward')\n",
    "\n",
    "    # Create a dendrogram\n",
    "    dendrogram(linkage_matrix)\n",
    "\n",
    "    # Plot the dendrogram (optional)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.show()\n",
    "\n",
    "        # Set a threshold or number of clusters based on your visual inspection\n",
    "    threshold = 1.4  # Adjust this value as needed\n",
    "\n",
    "    # Cut the dendrogram to obtain cluster labels\n",
    "    cluster_labels = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "    cluster_labels\n",
    "\n",
    "    # Create a dictionary to associate file names with cluster labels\n",
    "    clustered_data = {}\n",
    "    clustered_embeddings = {}\n",
    "    for i, (file_name, embedding) in enumerate(embeddings_dict.items()):\n",
    "        cluster_label = cluster_labels[i]\n",
    "        if cluster_label not in clustered_data:\n",
    "            clustered_data[cluster_label] = []\n",
    "            clustered_embeddings[cluster_label] = []\n",
    "        clustered_data[cluster_label].append(file_name)\n",
    "        clustered_embeddings[cluster_label].append(embedding)\n",
    "        \n",
    "        #system_utils.delete_specific_file(file_name)\n",
    "\n",
    "    # Now, clustered_data contains clusters as keys and lists of file names as values\n",
    "    for i in clustered_data:\n",
    "        print(\"cluster: \", i, \" \",clustered_data[i],\"\\n\")\n",
    "    \n",
    "    return clustered_embeddings\n",
    "\n",
    "clustered_embeddings = init_embeddings_cluster(CLUSTER_ROOT, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:  1   ['enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__1_20230906_200743.wav', 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__1_20230906_200819.wav', 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__2_20230906_200807.wav'] \n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Cosine Similarity: 0.4007475\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity\n",
    "#similarity = cosine_similarity([clustered_embeddings[0]], [clustered_embeddings[1]])\n",
    "emb_to_compare = clustered_embeddings[2]\n",
    "emb_to_compare_length = len(emb_to_compare)\n",
    "# The similarity variable now contains the cosine similarity value between array1 and array2\n",
    "#print(\"Cosine Similarity:\", similarity[0][0])\n",
    "\n",
    "for i, (k, embs) in enumerate(clustered_embeddings.items()):\n",
    "    print(file)\n",
    "    #print(i,\"\",clustered_embeddings[k],\"\\n\")\n",
    "    similarity = cosine_similarity(np.array(list(clustered_embeddings[1])), np.array(list(clustered_embeddings[2])))\n",
    "    print(\"Cosine Similarity:\", similarity[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "#def get_embedding_sample(embedding_list_1, embedding_list_2):\n",
    "#    l_3 = []\n",
    "#    # Check if list_1 is bigger than list_2\n",
    "#    if len(embedding_list_1) > len(embedding_list_2):\n",
    "#        # Randomly split list_1 into two parts of the same length as list_2\n",
    "#        l_3 = random.sample(embedding_list_1, len(embedding_list_2))  # Randomly select elements from list_1\n",
    "#    else:\n",
    "#        # Randomly grow list_1 to the size of list_2 based on its own elements\n",
    "#        l_3 = random.choices(embedding_list_1, k=len(embedding_list_2)) \n",
    "#    return l_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape : 2\n",
      " v shape (1, 512) - v_1 shape (3, 512)\n",
      "Cosine Similarities: [0.2108496]\n",
      "speaker  -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_01__3_20230906_200923.wav'\n",
    "#file_path = 'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__3_20230906_200854.wav'\n",
    "\n",
    "def get_speaker_key(embedding_model, clustered_embeddings_dict, file_path):\n",
    "    # Compute cosine similarity for each pair of arrays\n",
    "    dict_after_clustering = {}\n",
    "\n",
    "    dict_after_clustering[2] = pyannote.get_speaker_embedding_vector(embedding_model, file_path)#clustered_embeddings[3]\n",
    "    similarities = []\n",
    "    #identity_cluster = -1\n",
    "    #hihghest_similarity_score = -1\n",
    "    for _, (_, v) in enumerate(dict_after_clustering.items()):\n",
    "        for _, (_, v_1) in enumerate(clustered_embeddings_dict.items()):\n",
    "            v = np.array(list(v))\n",
    "            v_1 = np.array(list(v_1))\n",
    "            if len(v.shape) < 2:\n",
    "                v = v.reshape(1, -1)\n",
    "            if len(v_1.shape) < 2:\n",
    "                v_1 = v_1.reshape(1, -1)\n",
    "            print(f\"shape : {len(v_1.shape)}\")\n",
    "            print(f\" v shape {v.reshape(1, -1).shape} - v_1 shape {v_1.shape}\")\n",
    "            \n",
    "            similarity = cosine_similarity(v, v_1)[0][0]\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    # The similarities list now contains the cosine similarities for each pair of arrays\n",
    "    print(\"Cosine Similarities:\", similarities)\n",
    "    max_index = np.argmax(similarities)\n",
    "    if similarities[max_index] < 0.6:\n",
    "        max_index = -1 \n",
    "    print(\"speaker \", max_index)\n",
    "    return max_index\n",
    "\n",
    "get_speaker_key(embedding_model, clustered_embeddings, file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>total_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recorded_audio_20230906_164623.wav</td>\n",
       "      <td>1.026</td>\n",
       "      <td>4.512</td>\n",
       "      <td>9.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recorded_audio_20230906_164623.wav</td>\n",
       "      <td>5.175</td>\n",
       "      <td>9.985</td>\n",
       "      <td>9.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file_name  start   stop  total_length\n",
       "0  recorded_audio_20230906_164623.wav  1.026  4.512         9.985\n",
       "1  recorded_audio_20230906_164623.wav  5.175  9.985         9.985"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_utils.speach_activity_detection('enhanced_audio/enhanced/recorded_audio_20230906_164623.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>3.51</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.51</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  stop     speaker\n",
       "0   1.00  3.51  SPEAKER_00\n",
       "1   3.51  9.99  SPEAKER_01"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'enhanced_audio/enhanced/recorded_audio_20230906_164623.wav'\n",
    "diarization = pyannote.get_diarization_speaker_info_df(pyannote_pipeline, test)\n",
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seek</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>temperature</th>\n",
       "      <th>avg_logprob</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>no_speech_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>a very, very respected school.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.459736</td>\n",
       "      <td>1.243243</td>\n",
       "      <td>0.136985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Yeah, my wife and I, we lived in Plano for fi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.459736</td>\n",
       "      <td>1.243243</td>\n",
       "      <td>0.136985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>and my wife's from Houston.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.459736</td>\n",
       "      <td>1.243243</td>\n",
       "      <td>0.136985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>I mean, she, we made it.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.459736</td>\n",
       "      <td>1.243243</td>\n",
       "      <td>0.136985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  seek  start   end                                               text   \n",
       "0   0     0    0.0   5.0                     a very, very respected school.  \\\n",
       "1   1     0    5.0   8.0   Yeah, my wife and I, we lived in Plano for fi...   \n",
       "2   2     0    8.0   9.0                        and my wife's from Houston.   \n",
       "3   3     0    9.0  10.0                           I mean, she, we made it.   \n",
       "\n",
       "   temperature  avg_logprob  compression_ratio  no_speech_prob  \n",
       "0          0.0    -0.459736           1.243243        0.136985  \n",
       "1          0.0    -0.459736           1.243243        0.136985  \n",
       "2          0.0    -0.459736           1.243243        0.136985  \n",
       "3          0.0    -0.459736           1.243243        0.136985  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test ='enhanced_audio/enhanced/recorded_audio_20230906_164623.wav' #'enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__2_20230906_191648.wav'\n",
    "transcript_obj = whisper.get_transcription_object(transcription_model, test)\n",
    "transcript_obj[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_start</th>\n",
       "      <th>speaker_end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>max_start</th>\n",
       "      <th>min_end</th>\n",
       "      <th>overlap_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>a very, very respected school.</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.51</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Yeah, my wife and I, we lived in Plano for fi...</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>and my wife's from Houston.</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>I mean, she, we made it.</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.99</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  start   end                                               text   \n",
       "0   0    0.0   5.0                     a very, very respected school.  \\\n",
       "2   1    5.0   8.0   Yeah, my wife and I, we lived in Plano for fi...   \n",
       "3   2    8.0   9.0                        and my wife's from Houston.   \n",
       "4   3    9.0  10.0                           I mean, she, we made it.   \n",
       "\n",
       "   speaker_start  speaker_end     speaker  max_start  min_end   \n",
       "0           1.00         3.51  SPEAKER_00        1.0     3.51  \\\n",
       "2           3.51         9.99  SPEAKER_01        5.0     8.00   \n",
       "3           3.51         9.99  SPEAKER_01        8.0     9.00   \n",
       "4           3.51         9.99  SPEAKER_01        9.0     9.99   \n",
       "\n",
       "   overlap_duration  \n",
       "0              2.51  \n",
       "2              3.00  \n",
       "3              1.00  \n",
       "4              0.99  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_overlaps = transcription_without_overlapping_speakers(diarization, transcript_obj)\n",
    "all_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "import os\n",
    "\n",
    "sample_rate = 16000\n",
    "# Define parameters\n",
    "window_size = int(sample_rate * 0.03)  # 30 ms window size\n",
    "overlap = int(sample_rate * 0.015)  # 15 ms overlap\n",
    "\n",
    "# Define and load a pre-trained d-vector extraction model\n",
    "d_vector_model = torch.load('d_vector_model.pth')\n",
    "\n",
    "# Directory containing your audio clips\n",
    "audio_dir = 'enhanced_audio/chunks/'\n",
    "\n",
    "# List of audio clip filenames\n",
    "audio_files = os.listdir(audio_dir)\n",
    "\n",
    "# Initialize a dictionary to store d-vectors for each clip\n",
    "d_vectors_dict = {}\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    # Load the audio clip\n",
    "    audio_path = os.path.join(audio_dir, audio_file)\n",
    "    audio, sample_rate = librosa.load(audio_path, sr=None)\n",
    "    print(sample_rate)\n",
    "\n",
    "    # Divide the audio into overlapping frames\n",
    "    frames = librosa.util.frame(audio, frame_length=window_size, hop_length=overlap)\n",
    "\n",
    "    # Extract MFCC features from each frame\n",
    "    mfcc_features = librosa.feature.mfcc(y=frames, sr=sample_rate)\n",
    "\n",
    "    # Extract d-vectors from MFCC features\n",
    "    d_vectors = []\n",
    "    for frame_mfcc in mfcc_features.T:\n",
    "        frame_mfcc = torch.Tensor(frame_mfcc)\n",
    "        d_vector = d_vector_model(frame_mfcc)\n",
    "        d_vectors.append(d_vector)\n",
    "\n",
    "    # Perform pooling (e.g., mean pooling) on d-vectors within the clip\n",
    "    pooled_d_vector = torch.mean(torch.stack(d_vectors), dim=0)\n",
    "\n",
    "    # Store the pooled d-vector in the dictionary\n",
    "    d_vectors_dict[audio_file] = pooled_d_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9946786365975026"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa \n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "audio_1 = 'archive/16000_pcm_speeches/audio/Benjamin_Netanyau/10.wav'\n",
    "audio_2 = 'archive/16000_pcm_speeches/audio/Jens_Stoltenberg/30.wav'\n",
    "num_mfcc = 13\n",
    "#embedding = pyannote.get_embedding_model(PYANNOTE_ACCESS_TOKEN)\n",
    "y_1, sr_1 = librosa.load(audio_1, sr=None)\n",
    "y_2, sr_2 = librosa.load(audio_2, sr=None)\n",
    "\n",
    "mfccs_1 = librosa.feature.mfcc(y=y_1, sr=sr_1, n_mfcc=num_mfcc)\n",
    "mfccs_2 = librosa.feature.mfcc(y=y_2, sr=sr_2, n_mfcc=num_mfcc)\n",
    "\n",
    "mfccs_1 = mfccs_1.T\n",
    "mfccs_2 = mfccs_2.T\n",
    "\n",
    "eulr = 1 - cdist(mfccs_1, mfccs_2, metric=\"cosine\")[0,0]\n",
    "eulr\n",
    "\n",
    "#diff = pyannote.embedding_cosine_similarity(embedding, audio_1, audio_2)\n",
    "#diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src.utils.audio_utils as audio_utils\n",
    "SPEAKER_CLUSTER_LOCATION = 'enhanced_audio/test_clips/'\n",
    "\n",
    "content = os.listdir('enhanced_audio/chunks/')\n",
    "for path in content:\n",
    "    print(path)\n",
    "    path_item = os.path.join('enhanced_audio/chunks/', path)\n",
    "    audio_utils.enhance_audio_signal(path_item, f'enhanced_audio/enhanced/{path}')\n",
    "    diarization = pyannote.get_diarization_speaker_info_df(pyannote_pipeline, f'enhanced_audio/enhanced/{path}')\n",
    "    pyannote.cluster_audio_chunks(diarization, SPEAKER_CLUSTER_LOCATION, f'enhanced_audio/enhanced/{path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\adamb\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\ea322af152f0db03fcd5fd3d170ca28dc14c72a9\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio.pipelines import OverlappedSpeechDetection\n",
    "from pyannote.audio import Model\n",
    "\n",
    "dotenv_path = 'venv/env_variables.env'\n",
    "TOKEN = pyannote.get_pyannote_access_token(dotenv_path)\n",
    "model = Model.from_pretrained(\"pyannote/segmentation\", \n",
    "                              use_auth_token=TOKEN)\n",
    "HYPER_PARAMETERS = {\n",
    "  # onset/offset activation thresholds\n",
    "  \"onset\": 0.5, \"offset\": 0.5,\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "\n",
    "pipeline = OverlappedSpeechDetection(segmentation=model)\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "osd = pipeline(\"enhanced_audio/test_clips/recorded_audio_crop_SPEAKER_00__2_20230906_182427.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x22688720110>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = 'enhanced_audio/chunks/'\n",
    "folder_contents = os.listdir(folder_path)\n",
    "diarization_dict = {}\n",
    "global_counter = 0.0\n",
    "for i, item in enumerate(folder_contents):\n",
    "    item_path = os.path.join(folder_path, item)\n",
    "    audio_utils.enhance_audio_signal(item_path, f'enhanced_audio/enhanced/{item}')\n",
    "    diarization_dict = pyannote.get_diarization_speaker_info_df(pyannote_pipeline, f'enhanced_audio/enhanced/{item}')\n",
    "    print(f'file: {item} dict: {diarization_dict}\\n')\n",
    "    for _, row in diarization_dict.iterrows():\n",
    "        audio_utils.crop_wav(item_path, f'enhanced_audio/test_clips/{item}_{row[\"speaker\"]}_{(float(row[\"stop\"]) + global_counter)}.wav', float(row[\"start\"]), float(row[\"stop\"]))\n",
    "    #audio_utils.c\n",
    "    global_counter += 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = pyannote.get_embedding_model(PYANNOTE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pydub import AudioSegment\n",
    "\n",
    "cluster_file = 'enhanced_audio/test_clips/'\n",
    "folder_contents = os.listdir(cluster_file)\n",
    "speakers_embeddings_list = {}\n",
    "for i, item in enumerate(folder_contents):\n",
    "    item_path = os.path.join(cluster_file, item)\n",
    "    print(\"============> \",item_path)\n",
    "    #audio = AudioSegment.from_file(item_path)\n",
    "    #length_in_seconds = len(audio) / 1000.0\n",
    "    \n",
    "    #if length_in_seconds >= 1:\n",
    "    #    print(length_in_seconds)\n",
    "    speaker_embedding = pyannote.get_speaker_embedding_vector(embedding_model, item_path)\n",
    "    #speaker_embedding = mfcc_vectors[i]\n",
    "    speakers_embeddings_list[item_path] = speaker_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from pydub import AudioSegment\n",
    "\n",
    "cluster_file = 'enhanced_audio/test_clips/'\n",
    "folder_contents = os.listdir(cluster_file)\n",
    "seen = []\n",
    "comp = {}\n",
    "for i, item in enumerate(folder_contents):\n",
    "    item_path = os.path.join(cluster_file, item)\n",
    "    #emb_1 = speakers_embeddings_list[item_path]\n",
    "    folder_contents_y = os.listdir(cluster_file)\n",
    "    for y, item_y in enumerate(folder_contents_y):\n",
    "        current_path = os.path.join(cluster_file, item_y)\n",
    "        audio = AudioSegment.from_file(item_path)\n",
    "        length_in_seconds = len(audio) / 1000.0\n",
    "        if (item_y in seen) or (i == y) or (length_in_seconds < 1):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            #emb_1 = pyannote.get_speaker_embedding_vector(embedding_model, item_path)\n",
    "            #emb_2 = pyannote.get_speaker_embedding_vector(embedding_model, current_path)\n",
    "            #similarity = 1 - cdist(embedding1.reshape(1, -1), embedding2.reshape(1, -1), metric=\"cosine\")[0,0]\n",
    "            similarity = pyannote.embedding_cosine_similarity(embedding_model, item_path, current_path)\n",
    "            if similarity > 0.65:\n",
    "                if item_path not in comp:\n",
    "                    comp[item_path] = []\n",
    "                    comp[item_path].append(current_path)\n",
    "                else:\n",
    "                    comp[item_path].append(current_path)\n",
    "    seen.append(item_path)\n",
    "    #system_utils.delete_specific_file(item_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diarization based uterrances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_mfcc = 13  # Number of MFCC coefficients to extract\n",
    "frame_length = 25  # Frame length in milliseconds\n",
    "hop_length = 10    # Hop length in milliseconds\n",
    "\n",
    "def extract_mfcc(audio_file):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=num_mfcc, hop_length=int(hop_length * sr / 1000),\n",
    "                                 n_fft=int(frame_length * sr / 1000))\n",
    "\n",
    "    # Transpose the MFCC matrix to have time frames as rows\n",
    "    mfccs = mfccs.T\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "audio_dir = 'enhanced_audio/chunks/'  # Replace with the directory containing your audio files\n",
    "\n",
    "mfcc_file_pairs = []\n",
    "\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_file = os.path.join(audio_dir, filename)\n",
    "        mfcc = extract_mfcc(audio_file)\n",
    "        #print(f\"mfcc len {len(mfcc)}\")\n",
    "        #mfcc_file_pairs.append((mfcc, audio_file))\n",
    "\n",
    "        for i, vector in enumerate(mfcc):\n",
    "            mfcc_file_pairs.append((vector, audio_file))\n",
    "\n",
    "# Concatenate the MFCC vectors into a single NumPy array\n",
    "mfcc_data = np.vstack([pair[0] for pair in mfcc_file_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_file_pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Concatenate the MFCC vectors into a single NumPy array, which will serve as the input data for DBSCAN:\n",
    "#mfcc_data = np.vstack(mfcc_vectors)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mfcc_data_normalized = scaler.fit_transform(mfcc_data)\n",
    "\n",
    "eps = 0.55  # Adjust as needed\n",
    "min_samples = 5  # Adjust as needed\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(mfcc_data_normalized)\n",
    "#print(len(labels))\n",
    "\n",
    "\n",
    "# Create a dictionary to map labels to audio files\n",
    "label_to_audio_file = {}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    #print(f\"index {i}\")\n",
    "    audio_file = mfcc_file_pairs[i][1]\n",
    "    #print(\"audio ___> \",audio_file)\n",
    "    if label in label_to_audio_file:\n",
    "        label_to_audio_file[label].append(audio_file)\n",
    "    else:\n",
    "        label_to_audio_file[label] = [audio_file]\n",
    "\n",
    "#for label, audio_file in zip(labels, [pair[1] for pair in mfcc_file_pairs]):\n",
    "#    print(\"in label \", label)\n",
    "#    if label in label_to_audio_file:\n",
    "        \n",
    "#        label_to_audio_file[label].append(audio_file)\n",
    "#    else:\n",
    "#        label_to_audio_file[label] = [audio_file]\n",
    "\n",
    "# Plot the clustered data\n",
    "unique_labels = np.unique(labels)\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        # Plot noise points in black\n",
    "        plt.scatter(mfcc_data_normalized[labels == label, 0], mfcc_data_normalized[labels == label, 1], c='black', marker='o', s=10)\n",
    "    else:\n",
    "        # Plot data points belonging to clusters with different colors\n",
    "        plt.scatter(mfcc_data_normalized[labels == label, 0], mfcc_data_normalized[labels == label, 1], label=f'Cluster {label}')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('DBSCAN Clustering of MFCC Vectors')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced_audio/chunks/recorded_audio_20230905_205840.wav: 3\n",
      "enhanced_audio/chunks/recorded_audio_20230905_205830.wav: 2\n",
      "enhanced_audio/chunks/recorded_audio_20230905_205810.wav: 1\n",
      "enhanced_audio/chunks/recorded_audio_20230905_205910.wav: 1\n",
      "enhanced_audio/chunks/recorded_audio_20230905_205911.wav: 1\n"
     ]
    }
   ],
   "source": [
    "#label_to_audio_file[0]\n",
    "\n",
    "# Use numpy's unique function with return_counts=True\n",
    "unique_elements, frequencies = np.unique(label_to_audio_file[0], return_counts=True)\n",
    "\n",
    "# Create a dictionary to store the frequencies\n",
    "frequency_dict = dict(zip(unique_elements, frequencies))\n",
    "\n",
    "# Sort the dictionary by frequency in descending order\n",
    "sorted_frequency_dict = dict(sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the frequencies in descending order\n",
    "for element, frequency in sorted_frequency_dict.items():\n",
    "    print(f\"{element}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Concatenate the MFCC vectors into a single NumPy array, which will serve as the input data for DBSCAN:\n",
    "mfcc_data = np.vstack(mfcc_vectors)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mfcc_data_normalized = scaler.fit_transform(mfcc_data)\n",
    "\n",
    "eps = 0.7  # Adjust as needed\n",
    "min_samples = 5  # Adjust as needed\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(mfcc_data_normalized)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the clustered data\n",
    "unique_labels = np.unique(labels)\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        # Plot noise points in black\n",
    "        plt.scatter(mfcc_data_normalized[labels == label, 0], mfcc_data_normalized[labels == label, 1], c='black', marker='o', s=10)\n",
    "    else:\n",
    "        # Plot data points belonging to clusters with different colors\n",
    "        plt.scatter(mfcc_data_normalized[labels == label, 0], mfcc_data_normalized[labels == label, 1], label=f'Cluster {label}')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('DBSCAN Clustering of MFCC Vectors')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "# Standardize the data to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(list(speakers_embeddings_list.values()))\n",
    "\n",
    "\n",
    "\n",
    "dict_ = {}\n",
    "item_1 = X_standardized#list(speakers_embeddings_list.values())[0]\n",
    "len(X_standardized[0])\n",
    "#dict_[len(dict_)] = item_1\n",
    "#for key, embedding in X_standardized:#speakers_embeddings_list.items():\n",
    "#    distance = 1 - cdist(item_1.reshape(1, -1), embedding.reshape(1, -1), metric=\"cosine\")[0,0]\n",
    "    #sim = 1 - pyannote.embedding_cosine_similarity(speakers_embeddings_list[0], dict_[i])\n",
    "#    print(f\"{key} sim with index {0}, similarity: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# Standardize the data to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "item_1 = item_1 * 100000\n",
    "X_standardized = scaler.fit_transform(list(speakers_embeddings_list.values()))\n",
    "X_standardized\n",
    "\n",
    "# Create the PCA model with 95% variance retention\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Fit the PCA model to the standardized data\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "X_pca\n",
    "\n",
    "#item_1.reshape(1, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transcription based uterrances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a DBSCAN instance\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Fit the DBSCAN model to your data\n",
    "clusters = dbscan.fit_predict(list(speakers_embeddings_list.values()))\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
