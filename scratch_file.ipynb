{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.audio_utils import record_audio_chunk, concatenate_stream, download_mp3, start_continuous_recording, enhance_audio_signal, speach_activity_detection\n",
    "\n",
    "\n",
    "from src.utils.system_utils import create_folder_structure, delete_chunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.feature\n",
    "import os\n",
    "\n",
    "# Path to the folder containing audio slice files\n",
    "slices_folder = 'audio/chunks/'\n",
    "\n",
    "# Get a list of all audio slice files in the folder\n",
    "slice_files = [f for f in os.listdir(slices_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "# Initialize a list to store MFCC features\n",
    "mfcc_features = []\n",
    "\n",
    "# Loop through each slice file and extract MFCC features\n",
    "for slice_file in slice_files:\n",
    "    slice_path = os.path.join(slices_folder, slice_file)\n",
    "    audio_samples, sample_rate = librosa.load(slice_path, sr=None)\n",
    "    \n",
    "    # Extract MFCC features for the current slice\n",
    "    mfcc = librosa.feature.mfcc(y=audio_samples, sr=sample_rate, n_mfcc=13)\n",
    "    mfcc_features.append(mfcc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Testing different clustering methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering with Cosine Similarity\n",
    "We will use hierarchical agglomerative clustering with cosine similarity to cluster the audio features. This allows us to handle an unknown number of speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your MFCC features as mfcc_features (a list of arrays)\n",
    "# Concatenate all MFCC features to form a feature matrix\n",
    "feature_matrix = np.concatenate(mfcc_features, axis=1)\n",
    "\n",
    "# Calculate the distance matrix using cosine similarity\n",
    "# You might need to normalize your features before calculating cosine similarity\n",
    "# Normalize the feature matrix using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_feature_matrix = scaler.fit_transform(feature_matrix)\n",
    "cosine_sim = 1 - np.dot(normalized_feature_matrix, normalized_feature_matrix.T)\n",
    "\n",
    "# Perform hierarchical agglomerative clustering\n",
    "linked = linkage(cosine_sim, method='ward')\n",
    "\n",
    "# Plot the dendrogram (optional)\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Determine the optimal number of clusters using a distance threshold\n",
    "distance_threshold = 0.4  # You can experiment with different thresholds\n",
    "cluster_labels = fcluster(linked, distance_threshold, criterion='distance')\n",
    "\n",
    "# Print the cluster labels\n",
    "print(\"Cluster labels:\", cluster_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN \n",
    "DBSCAN is particularly useful when the number of clusters is not known in advance and can handle various shapes of clusters, including handling noise points effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Concatenate your MFCC features into a feature matrix\n",
    "feature_matrix = np.concatenate(mfcc_features, axis=1)\n",
    "\n",
    "# Normalize the feature matrix using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_feature_matrix = scaler.fit_transform(feature_matrix)\n",
    "\n",
    "# Initialize DBSCAN with appropriate parameters\n",
    "eps = 0.5  # Adjust the epsilon (neighborhood distance) based on your data\n",
    "min_samples = 5  # Adjust the minimum number of samples in a neighborhood\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "\n",
    "# Fit DBSCAN\n",
    "cluster_labels = dbscan.fit_predict(normalized_feature_matrix)\n",
    "\n",
    "# Print the cluster labels\n",
    "print(\"Cluster labels:\", cluster_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Cluster Validation (Silhouette Score - Same as before)\n",
    "##### Calculate the Silhouette Score to evaluate the quality of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the Silhouette Score\n",
    "silhouette_avg = silhouette_score(feature_matrix, cluster_labels)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Speaker Diarization with Speaker Tracking\n",
    "##### To perform speaker diarization and remember speakers across different time slices, we will use a tracking mechanism based on the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the speakers and their corresponding segments\n",
    "speakers_dict = {}\n",
    "\n",
    "# Loop through each segment and track the speakers\n",
    "for idx, label in enumerate(cluster_labels):\n",
    "    if label not in speakers_dict:\n",
    "        speakers_dict[label] = [idx]\n",
    "    else:\n",
    "        speakers_dict[label].append(idx)\n",
    "\n",
    "# Print the diarization results\n",
    "for speaker, segments in speakers_dict.items():\n",
    "    print(f\"Speaker {speaker}: Segments {segments}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "folder_path = 'test'\n",
    "subfolder_name = 'audio'\n",
    "full_subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(full_subfolder_path):\n",
    "    # If it exists, delete the folder and its contents\n",
    "    shutil.rmtree(folder_path)\n",
    "    \n",
    "# Create the 'test' folder and 'audio' subfolder\n",
    "os.makedirs(full_subfolder_path)\n",
    "\n",
    "print(\"Folder structure created successfully.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### delete file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "parent_folder = 'test'\n",
    "subfolder = 'audio'\n",
    "file_to_delete = 'example.txt'  # file to delete\n",
    "\n",
    "file_path = os.path.join(parent_folder, subfolder, file_to_delete)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_to_delete}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"File '{file_to_delete}' does not exist.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### speach detection \n",
    "\n",
    "We will need to make sure that these speaker turns match with the transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "import os\n",
    "import pandas as pd\n",
    "from src.utils.audio_utils import speach_activity_detection \n",
    "\n",
    "#f = 'audio/chunks/recorded_audio_20230819_231924.wav'\n",
    "#enhance_audio_signal(audio_path=f)\n",
    "\n",
    "#adjust target amplitude\n",
    "\n",
    "# Path to the folder containing audio segments\n",
    "segments_folder = 'test/audio/temp/temp_speaker_segements/SPEAKER_01/recorded_audio_crop_0.wav'\n",
    "df = speach_activity_detection(filename=segments_folder)\n",
    "df.iloc[:][['start', 'stop']].values[0][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clustering k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_folder = 'audio/chunks/'\n",
    "files_path_list = []    \n",
    "for filename in os.listdir(segments_folder):\n",
    "    if filename.endswith('.wav'):\n",
    "        segment_path = os.path.join(segments_folder, filename)\n",
    "        files_path_list.append(segment_path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to extract MFCC features for a given audio file\n",
    "def extract_mfcc(audio_file):\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    return mfcc.T  # Transpose to have time on the x-axis\n",
    "\n",
    "# Directory containing your 10-second audio files\n",
    "audio_dir = 'audio/chunks/'\n",
    "\n",
    "# List audio files in the directory\n",
    "audio_files = [os.path.join(audio_dir, filename) for filename in os.listdir(audio_dir) if filename.endswith('.wav')]\n",
    "\n",
    "# Extract MFCC features for all audio files\n",
    "all_mfcc = np.vstack([extract_mfcc(audio_file) for audio_file in audio_files])\n",
    "\n",
    "# Perform PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_mfcc = pca.fit_transform(all_mfcc)\n",
    "\n",
    "# Perform K-means clustering\n",
    "num_clusters = 4  # You can adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "cluster_labels = kmeans.fit_predict(reduced_mfcc)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(reduced_mfcc[:, 0], reduced_mfcc[:, 1], c=cluster_labels, cmap='rainbow')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "plt.title('K-means Clustering of Audio Segments')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diarization test pyAudioAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load your audio file\n",
    "audio_file = \"audio/chunks/recorded_audio_20230820_030026.wav\"\n",
    "\n",
    "# Perform speaker diarization\n",
    "result = aS.speaker_diarization(audio_file, n_speakers=2)\n",
    "\n",
    "# Extract the segments from the result\n",
    "segments = result\n",
    "\n",
    "# Detect overlapping segments\n",
    "overlapping_segments = []\n",
    "\n",
    "for i in range(len(segments) - 1):\n",
    "    #print(f\"segemnt: {segments[i]}\")\n",
    "    print(f\"segement 1: {segments[2]} \\n and segment 2: {segments[i + 1]} \\n =================================================================\")\n",
    "    end_time_speaker1, start_time_speaker2 = segments[i][1], segments[i + 1][0]\n",
    "    \n",
    "    if end_time_speaker1 > start_time_speaker2:\n",
    "        overlapping_segments.append((start_time_speaker2, end_time_speaker1))\n",
    "\n",
    "# Plot the waveform of the audio\n",
    "audio, _ = aS.readAudioFile(audio_file)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.linspace(0, len(audio) / 44100, num=len(audio)), audio)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.title(\"Audio Waveform\")\n",
    "\n",
    "# Highlight overlapping segments\n",
    "for segment in overlapping_segments:\n",
    "    plt.axvspan(segment[0], segment[1], color='red', alpha=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print or visualize overlapping segments\n",
    "for i, segment in enumerate(overlapping_segments):\n",
    "    print(f\"Overlap {i+1} at {segment[0]} to {segment[1]} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
