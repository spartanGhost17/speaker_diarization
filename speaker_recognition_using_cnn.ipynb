{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MeCiUmPlUmom"
      },
      "source": [
        "# Speaker Recognition\n",
        "\n",
        "**Author:** [Fadi Badine](https://twitter.com/fadibadine)<br>\n",
        "**Date created:** 14/06/2020<br>\n",
        "**Last modified:** 03/07/2020<br>\n",
        "**Description:** Classify speakers using Fast Fourier Transform (FFT) and a 1D Convnet."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEb_s-eUmor"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to create a model to classify speakers from the\n",
        "frequency domain representation of speech recordings, obtained via Fast Fourier\n",
        "Transform (FFT).\n",
        "\n",
        "It shows the following:\n",
        "\n",
        "- How to use `tf.data` to load, preprocess and feed audio streams into a model\n",
        "- How to create a 1D convolutional network with residual\n",
        "connections for audio classification.\n",
        "\n",
        "Our process:\n",
        "\n",
        "- We prepare a dataset of speech samples from different speakers, with the speaker as label.\n",
        "- We add background noise to these samples to augment our data.\n",
        "- We take the FFT of these samples.\n",
        "- We train a 1D convnet to predict the correct speaker given a noisy FFT speech sample.\n",
        "\n",
        "Note:\n",
        "\n",
        "- This example should be run with TensorFlow 2.3 or higher, or `tf-nightly`.\n",
        "- The noise samples in the dataset need to be resampled to a sampling rate of 16000 Hz\n",
        "before using the code in this example. In order to do this, you will need to have\n",
        "installed `ffmpg`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BIpaleHPUmor"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SQkCnLhgUmos"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "# Get the data from https://www.kaggle.com/kongaevans/speaker-recognition-dataset/download\n",
        "# and save it to the 'Downloads' folder in your HOME directory\n",
        "DATASET_ROOT = os.path.join(os.path.expanduser(\"~\"), \"DevProjects/AI/speaker_diarization/archive/16000_pcm_speeches\")\n",
        "\n",
        "# The folders in which we will put the audio samples and the noise samples\n",
        "AUDIO_SUBFOLDER = \"audio\"\n",
        "NOISE_SUBFOLDER = \"noise\"\n",
        "\n",
        "DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n",
        "DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)\n",
        "\n",
        "# Percentage of samples to use for validation\n",
        "VALID_SPLIT = 0.1\n",
        "\n",
        "# Seed to use when shuffling the dataset and the noise\n",
        "SHUFFLE_SEED = 43\n",
        "\n",
        "# The sampling rate to use.\n",
        "# This is the one used in all the audio samples.\n",
        "# We will resample all the noise to this sampling rate.\n",
        "# This will also be the output size of the audio wave samples\n",
        "# (since all samples are of 1 second long)\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "# The factor to multiply the noise with according to:\n",
        "#   noisy_sample = sample + noise * prop * scale\n",
        "#      where prop = sample_amplitude / noise_amplitude\n",
        "SCALE = 0.5\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bRA_yewKUmot"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "The dataset is composed of 7 folders, divided into 2 groups:\n",
        "\n",
        "- Speech samples, with 5 folders for 5 different speakers. Each folder contains\n",
        "1500 audio files, each 1 second long and sampled at 16000 Hz.\n",
        "- Background noise samples, with 2 folders and a total of 6 files. These files\n",
        "are longer than 1 second (and originally not sampled at 16000 Hz, but we will resample them to 16000 Hz).\n",
        "We will use those 6 files to create 354 1-second-long noise samples to be used for training.\n",
        "\n",
        "Let's sort these 2 categories into 2 folders:\n",
        "\n",
        "- An `audio` folder which will contain all the per-speaker speech sample folders\n",
        "- A `noise` folder which will contain all the noise samples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UpHEs7bEUmou"
      },
      "source": [
        "Before sorting the audio and noise categories into 2 folders,\n",
        "we have the following directory structure:\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...speaker_a/\n",
        "...speaker_b/\n",
        "...speaker_c/\n",
        "...speaker_d/\n",
        "...speaker_e/\n",
        "...other/\n",
        "..._background_noise_/\n",
        "```\n",
        "\n",
        "After sorting, we end up with the following structure:\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...audio/\n",
        "......speaker_a/\n",
        "......speaker_b/\n",
        "......speaker_c/\n",
        "......speaker_d/\n",
        "......speaker_e/\n",
        "...noise/\n",
        "......other/\n",
        "......_background_noise_/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J-S1m6WEUmou"
      },
      "outputs": [],
      "source": [
        "# If folder `audio`, does not exist, create it, otherwise do nothing\n",
        "if os.path.exists(DATASET_AUDIO_PATH) is False:\n",
        "    os.makedirs(DATASET_AUDIO_PATH)\n",
        "\n",
        "# If folder `noise`, does not exist, create it, otherwise do nothing\n",
        "if os.path.exists(DATASET_NOISE_PATH) is False:\n",
        "    os.makedirs(DATASET_NOISE_PATH)\n",
        "\n",
        "for folder in os.listdir(DATASET_ROOT):\n",
        "    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n",
        "        if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:\n",
        "            # If folder is `audio` or `noise`, do nothing\n",
        "            continue\n",
        "        elif folder in [\"other\", \"_background_noise_\"]:\n",
        "            # If folder is one of the folders that contains noise samples,\n",
        "            # move it to the `noise` folder\n",
        "            shutil.move(\n",
        "                os.path.join(DATASET_ROOT, folder),\n",
        "                os.path.join(DATASET_NOISE_PATH, folder),\n",
        "            )\n",
        "        else:\n",
        "            # Otherwise, it should be a speaker folder, then move it to\n",
        "            # `audio` folder\n",
        "            shutil.move(\n",
        "                os.path.join(DATASET_ROOT, folder),\n",
        "                os.path.join(DATASET_AUDIO_PATH, folder),\n",
        "            )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-3O3JJrQUmov"
      },
      "source": [
        "## Noise preparation\n",
        "\n",
        "In this section:\n",
        "\n",
        "- We load all noise samples (which should have been resampled to 16000)\n",
        "- We split those noise samples to chunks of 16000 samples which\n",
        "correspond to 1 second duration each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r9vAJCnUUmov"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 files belonging to 2 directories\n"
          ]
        }
      ],
      "source": [
        "# Get the list of all noise files\n",
        "noise_paths = []\n",
        "for subdir in os.listdir(DATASET_NOISE_PATH):\n",
        "    subdir_path = Path(DATASET_NOISE_PATH) / subdir\n",
        "    if os.path.isdir(subdir_path):\n",
        "        noise_paths += [\n",
        "            os.path.join(subdir_path, filepath)\n",
        "            for filepath in os.listdir(subdir_path)\n",
        "            if filepath.endswith(\".wav\")\n",
        "        ]\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} directories\".format(\n",
        "        len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))\n",
        "    )\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3yKy020CUmow"
      },
      "source": [
        "Resample all noise samples to 16000 Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ehLg2ml8Umow"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here checking path C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\other\\exercise_bike.wav\n",
            "Sampling rate for C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\other\\exercise_bike.wav is incorrect. Ignoring it\n",
            "here checking path C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\other\\pink_noise.wav\n",
            "Sampling rate for C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\other\\pink_noise.wav is incorrect. Ignoring it\n",
            "here checking path C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\10convert.com_Audience-Claps_daSG5fwdA7o.wav\n",
            "Sampling rate for C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\10convert.com_Audience-Claps_daSG5fwdA7o.wav is incorrect. Ignoring it\n",
            "here checking path C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\doing_the_dishes.wav\n",
            "Sampling rate for C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\doing_the_dishes.wav is incorrect. Ignoring it\n",
            "here checking path C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\dude_miaowing.wav\n",
            "Sampling rate for C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\dude_miaowing.wav is incorrect. Ignoring it\n",
            "here checking path C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\running_tap.wav\n",
            "Sampling rate for C:\\Users\\adamb\\DevProjects\\AI\\speaker_diarization\\archive\\16000_pcm_speeches\\noise\\_background_noise_\\running_tap.wav is incorrect. Ignoring it\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 39\u001b[0m\n\u001b[0;32m     34\u001b[0m         noises\u001b[39m.\u001b[39mextend(sample)\n\u001b[0;32m     35\u001b[0m noises \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstack(noises)\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m noise files were split into \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m noise samples where each is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sec. long\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mlen\u001b[39m(noise_paths), noises\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], noises\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m SAMPLING_RATE\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:910\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    909\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v2_behavior:\n\u001b[1;32m--> 910\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dims[key]\n\u001b[0;32m    911\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims[key]\n",
            "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "command = (\n",
        "    \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"\n",
        "    \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"\n",
        "    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n",
        "    \"$file | grep sample_rate | cut -f2 -d=`; \"\n",
        "    \"if [ $sample_rate -ne 16000 ]; then \"\n",
        "    \"ffmpeg -hide_banner -loglevel panic -y \"\n",
        "    \"-i $file -ar 16000 temp.wav; \"\n",
        "    \"mv temp.wav $file; \"\n",
        "    \"fi; done; done\"\n",
        ")\n",
        "os.system(command)\n",
        "\n",
        "# Split noise into chunks of 16,000 steps each\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "    if sampling_rate == SAMPLING_RATE:\n",
        "        # Number of slices of 16000 each that can be generated from the noise sample\n",
        "        slices = int(sample.shape[0] / SAMPLING_RATE)\n",
        "        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n",
        "        return None\n",
        "\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    print(\"here checking path {}\".format(path))\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "noises = tf.stack(noises)\n",
        "\n",
        "print(\n",
        "    \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
        "        len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
        "    )\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tLlRDLHxUmow"
      },
      "source": [
        "## Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gdm4UXJUUmow"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our class names: ['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']\n",
            "Processing speaker Benjamin_Netanyau\n",
            "Processing speaker Jens_Stoltenberg\n",
            "Processing speaker Julia_Gillard\n",
            "Processing speaker Magaret_Tarcher\n",
            "Processing speaker Nelson_Mandela\n",
            "Found 7501 files belonging to 5 classes.\n",
            "Using 6751 files for training.\n",
            "Using 750 files for validation.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\adamb\\AppData\\Local\\Temp\\ipykernel_7804\\300507151.py\", line 100, in None  *\n        lambda x, y: (add_noise(x, noises, scale=SCALE), y)\n    File \"C:\\Users\\adamb\\AppData\\Local\\Temp\\ipykernel_7804\\300507151.py\", line 26, in add_noise  *\n        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n\n    ValueError: Invalid reduction dimension 1 for input with 1 dimensions. for '{{node Max_1}} = Max[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](GatherV2, Max_1/reduction_indices)' with input shapes: [?], [] and with computed input tensors: input[1] = <1>.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 99\u001b[0m\n\u001b[0;32m     95\u001b[0m valid_ds \u001b[39m=\u001b[39m valid_ds\u001b[39m.\u001b[39mshuffle(buffer_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m, seed\u001b[39m=\u001b[39mSHUFFLE_SEED)\u001b[39m.\u001b[39mbatch(\u001b[39m32\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Add noise to the training set\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m train_ds \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    100\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x, y: (add_noise(x, noises, scale\u001b[39m=\u001b[39;49mSCALE), y),\n\u001b[0;32m    101\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE,\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[39m# Transform audio wave to the frequency domain using `audio_to_fft`\u001b[39;00m\n\u001b[0;32m    105\u001b[0m train_ds \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    106\u001b[0m     \u001b[39mlambda\u001b[39;00m x, y: (audio_to_fft(x), y), num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE\n\u001b[0;32m    107\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2204\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2202\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2204\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2205\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2206\u001b[0m       map_func,\n\u001b[0;32m   2207\u001b[0m       num_parallel_calls,\n\u001b[0;32m   2208\u001b[0m       deterministic,\n\u001b[0;32m   2209\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2210\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5441\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m   5440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m-> 5441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m   5442\u001b[0m     map_func,\n\u001b[0;32m   5443\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[0;32m   5444\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[0;32m   5445\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[0;32m   5446\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5447\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2610\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2602\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2603\u001b[0m \n\u001b[0;32m   2604\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2608\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2610\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2611\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2612\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2576\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2576\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2577\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2578\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2579\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2670\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2667\u001b[0m ]\n\u001b[0;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2673\u001b[0m         args,\n\u001b[0;32m   2674\u001b[0m         kwargs,\n\u001b[0;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1247\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1247\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1249\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   1252\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m    243\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m    244\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[0;32m    245\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[0;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    249\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    176\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 177\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n\u001b[0;32m    179\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ret)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqeaqh3gz.py:5\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner_factory\u001b[39m(ag__):\n\u001b[1;32m----> 5\u001b[0m     tf__lam \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, y: ag__\u001b[39m.\u001b[39;49mwith_function_scope(\u001b[39mlambda\u001b[39;49;00m lscope: (ag__\u001b[39m.\u001b[39;49mconverted_call(add_noise, (x, noises), \u001b[39mdict\u001b[39;49m(scale\u001b[39m=\u001b[39;49mSCALE), lscope), y), \u001b[39m'\u001b[39;49m\u001b[39mlscope\u001b[39;49m\u001b[39m'\u001b[39;49m, ag__\u001b[39m.\u001b[39;49mSTD)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m tf__lam\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\core\\function_wrappers.py:113\u001b[0m, in \u001b[0;36mwith_function_scope\u001b[1;34m(thunk, scope_name, options)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Inline version of the FunctionScope context manager.\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m FunctionScope(\u001b[39m'\u001b[39m\u001b[39mlambda_\u001b[39m\u001b[39m'\u001b[39m, scope_name, options) \u001b[39mas\u001b[39;00m scope:\n\u001b[1;32m--> 113\u001b[0m   \u001b[39mreturn\u001b[39;00m thunk(scope)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqeaqh3gz.py:5\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[1;34m(lscope)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner_factory\u001b[39m(ag__):\n\u001b[1;32m----> 5\u001b[0m     tf__lam \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x, y: ag__\u001b[39m.\u001b[39mwith_function_scope(\u001b[39mlambda\u001b[39;00m lscope: (ag__\u001b[39m.\u001b[39;49mconverted_call(add_noise, (x, noises), \u001b[39mdict\u001b[39;49m(scale\u001b[39m=\u001b[39;49mSCALE), lscope), y), \u001b[39m'\u001b[39m\u001b[39mlscope\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mSTD)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m tf__lam\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5qs4nknj.py:32\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__add_noise\u001b[1;34m(audio, noises, scale)\u001b[0m\n\u001b[0;32m     30\u001b[0m prop \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mprop\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m noise \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(noises) \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m'\u001b[39;49m,), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1363\u001b[0m, in \u001b[0;36mif_stmt\u001b[1;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[0;32m   1361\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[0;32m   1362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1363\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1416\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[1;34m(cond, body, orelse)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[0;32m   1415\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1416\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5qs4nknj.py:22\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__add_noise.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m tf_rnd \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform, ((ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mshape, (ag__\u001b[39m.\u001b[39mld(audio),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m0\u001b[39m],), \u001b[39m0\u001b[39m, ag__\u001b[39m.\u001b[39mld(noises)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mint32), fscope)\n\u001b[0;32m     21\u001b[0m noise \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mgather, (ag__\u001b[39m.\u001b[39mld(noises), ag__\u001b[39m.\u001b[39mld(tf_rnd)), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), fscope)\n\u001b[1;32m---> 22\u001b[0m prop \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_max, (ag__\u001b[39m.\u001b[39mld(audio),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope) \u001b[39m/\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mreduce_max, (ag__\u001b[39m.\u001b[39;49mld(noise),), \u001b[39mdict\u001b[39;49m(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), fscope)\n\u001b[0;32m     23\u001b[0m prop \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mrepeat, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mexpand_dims, (ag__\u001b[39m.\u001b[39mld(prop),), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mshape, (ag__\u001b[39m.\u001b[39mld(audio),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m1\u001b[39m]), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fscope)\n\u001b[0;32m     24\u001b[0m audio \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(audio) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(noise) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(prop) \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(scale)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\adamb\\anaconda3\\envs\\tensorflow_conda_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1969\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1966\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1967\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1968\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1969\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m   1971\u001b[0m \u001b[39m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1972\u001b[0m \u001b[39m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1973\u001b[0m \u001b[39mif\u001b[39;00m extract_traceback:\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\adamb\\AppData\\Local\\Temp\\ipykernel_7804\\300507151.py\", line 100, in None  *\n        lambda x, y: (add_noise(x, noises, scale=SCALE), y)\n    File \"C:\\Users\\adamb\\AppData\\Local\\Temp\\ipykernel_7804\\300507151.py\", line 26, in add_noise  *\n        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n\n    ValueError: Invalid reduction dimension 1 for input with 1 dimensions. for '{{node Max_1}} = Max[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](GatherV2, Max_1/reduction_indices)' with input shapes: [?], [] and with computed input tensors: input[1] = <1>.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def paths_and_labels_to_dataset(audio_paths, labels):\n",
        "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
        "    return audio\n",
        "\n",
        "\n",
        "def add_noise(audio, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        # Create a random tensor of the same size as audio ranging from\n",
        "        # 0 to the number of noise stream samples that we have.\n",
        "        tf_rnd = tf.random.uniform(\n",
        "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
        "        )\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        # Get the amplitude proportion between the audio and the noise\n",
        "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
        "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
        "\n",
        "        # Adding the rescaled noise to audio\n",
        "        audio = audio + noise * prop * scale\n",
        "\n",
        "    return audio\n",
        "\n",
        "\n",
        "def audio_to_fft(audio):\n",
        "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
        "    # we need to squeeze the dimensions and then expand them again\n",
        "    # after FFT\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    # Return the absolute value of the first half of the FFT\n",
        "    # which represents the positive frequencies\n",
        "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
        "\n",
        "\n",
        "# Get the list of audio file paths along with their corresponding labels\n",
        "\n",
        "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
        "print(\"Our class names: {}\".format(class_names,))\n",
        "\n",
        "audio_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Processing speaker {}\".format(name,))\n",
        "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
        "    speaker_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    audio_paths += speaker_sample_paths\n",
        "    labels += [label] * len(speaker_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
        ")\n",
        "\n",
        "# Shuffle\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(audio_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Split into training and validation\n",
        "num_val_samples = int(VALID_SPLIT * len(audio_paths))\n",
        "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
        "train_audio_paths = audio_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "print(\"Using {} files for validation.\".format(num_val_samples))\n",
        "valid_audio_paths = audio_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]\n",
        "\n",
        "# Create 2 datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
        "\n",
        "\n",
        "# Add noise to the training set\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        ")\n",
        "\n",
        "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz8XKNsNUmox"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt07n1AQUmox"
      },
      "outputs": [],
      "source": [
        "\n",
        "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
        "    # Shortcut\n",
        "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
        "    for i in range(conv_num - 1):\n",
        "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.Activation(activation)(x)\n",
        "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
        "    x = keras.layers.Add()([x, s])\n",
        "    x = keras.layers.Activation(activation)(x)\n",
        "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
        "\n",
        "    x = residual_block(inputs, 16, 2)\n",
        "    x = residual_block(x, 32, 2)\n",
        "    x = residual_block(x, 64, 3)\n",
        "    x = residual_block(x, 128, 3)\n",
        "    x = residual_block(x, 128, 3)\n",
        "\n",
        "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
        "\n",
        "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model using Adam's default learning rate\n",
        "model.compile(\n",
        "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Add callbacks:\n",
        "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
        "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
        "model_save_filename = \"model.h5\"\n",
        "\n",
        "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xIplKi0_Umox"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY1xyo_DUmox"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rFWuDMFpUmox"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r2znpKqUmox"
      },
      "outputs": [],
      "source": [
        "print(model.evaluate(valid_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5g-6JGrtUmox"
      },
      "source": [
        "We get ~ 98% validation accuracy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ovUS2uFwUmox"
      },
      "source": [
        "## Demonstration\n",
        "\n",
        "Let's take some samples and:\n",
        "\n",
        "- Predict the speaker\n",
        "- Compare the prediction with the real speaker\n",
        "- Listen to the audio to see that despite the samples being noisy,\n",
        "the model is still pretty accurate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FXrB-13Umoy"
      },
      "outputs": [],
      "source": [
        "SAMPLES_TO_DISPLAY = 10\n",
        "\n",
        "test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
        "\n",
        "for audios, labels in test_ds.take(1):\n",
        "    # Get the signal FFT\n",
        "    ffts = audio_to_fft(audios)\n",
        "    # Predict\n",
        "    y_pred = model.predict(ffts)\n",
        "    # Take random samples\n",
        "    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n",
        "    audios = audios.numpy()[rnd, :, :]\n",
        "    labels = labels.numpy()[rnd]\n",
        "    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "    for index in range(SAMPLES_TO_DISPLAY):\n",
        "        # For every sample, print the true and predicted label\n",
        "        # as well as run the voice with the noise\n",
        "        print(\n",
        "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[labels[index]],\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[y_pred[index]],\n",
        "            )\n",
        "        )\n",
        "        display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "speaker_recognition_using_cnn",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
